from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
import pymysql
from datetime import datetime, timedelta
import os
import json
import logging
import math
from functools import lru_cache
import time
from scripts.prometheus_client import PrometheusClient
from scripts.sql_fingerprint import SQLFingerprint
from scripts.sql_explain_analyzer import SQLExplainAnalyzer
from scripts.sqlserver_deadlock_collector import collect_all_sqlserver_deadlocks
from apscheduler.schedulers.background import BackgroundScheduler
import atexit

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è¿æ¥æ± ï¼Œå¦‚æœæ²¡æœ‰å®‰è£…åˆ™ä½¿ç”¨æ™®é€šè¿æ¥
try:
    from DBUtils.PooledDB import PooledDB
    HAS_POOL = True
except ImportError:
    logger.warning("DBUtilsæœªå®‰è£…ï¼Œä½¿ç”¨æ™®é€šæ•°æ®åº“è¿æ¥ã€‚å»ºè®®å®‰è£…: pip install DBUtils")
    HAS_POOL = False

app = Flask(__name__, static_folder='static', static_url_path='', template_folder='templates')
CORS(app)

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.json')

# é»˜è®¤é…ç½®
DEFAULT_DB_CONFIG = {
    'host': '192.168.11.85',
    'port': 3306,
    'user': 'root',
    'password': 'Root#2025Jac.Com',
    'database': 'db_monitor',
    'charset': 'utf8mb4'
}

DEFAULT_APP_CONFIG = {
    'auto_refresh_interval': 30,
    'warning_threshold': 5,
    'critical_threshold': 10,
    'default_hours': 24,
    'default_page_size': 20
}

# é…ç½®ç¼“å­˜æ—¶é—´æˆ³ï¼Œç”¨äºæ‰‹åŠ¨æ¸…é™¤ç¼“å­˜
_config_cache_timestamp = time.time()

@lru_cache(maxsize=1)
def _load_config_cached(cache_key):
    """ä»æ–‡ä»¶åŠ è½½é…ç½®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        if os.path.exists(CONFIG_FILE):
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                config = json.load(f)
                logger.debug(f"ä»æ–‡ä»¶åŠ è½½é…ç½® (ç¼“å­˜é”®: {cache_key})")
                return config
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
    return {'database': DEFAULT_DB_CONFIG.copy(), 'app': DEFAULT_APP_CONFIG.copy()}

def load_config():
    """ä»æ–‡ä»¶åŠ è½½é…ç½®ï¼ˆè‡ªåŠ¨ç¼“å­˜60ç§’ï¼‰"""
    # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºç¼“å­˜é”®ï¼Œæ¯60ç§’è‡ªåŠ¨å¤±æ•ˆ
    cache_key = int(time.time() / 60)
    return _load_config_cached(cache_key)

def clear_config_cache():
    """æ‰‹åŠ¨æ¸…é™¤é…ç½®ç¼“å­˜"""
    _load_config_cached.cache_clear()
    logger.info("é…ç½®ç¼“å­˜å·²æ¸…é™¤")

def save_config(config):
    """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
    try:
        with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=4, ensure_ascii=False)
        # ä¿å­˜åæ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡è¯»å–æœ€æ–°é…ç½®
        clear_config_cache()
        return True
    except Exception as e:
        logger.error(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")
        return False

def get_db_config():
    """è·å–æ•°æ®åº“é…ç½®"""
    config = load_config()
    db_config = config.get('database', DEFAULT_DB_CONFIG)
    return {
        'host': os.getenv('DB_HOST', db_config.get('host', '127.0.0.1')),
        'port': int(os.getenv('DB_PORT', db_config.get('port', 3306))),
        'user': os.getenv('DB_USER', db_config.get('user', 'root')),
        'password': os.getenv('DB_PASSWORD', db_config.get('password', '')),
        'database': os.getenv('DB_NAME', db_config.get('database', 'db_monitor')),
        'charset': db_config.get('charset', 'utf8mb4'),
        'cursorclass': pymysql.cursors.DictCursor
    }

# å…¨å±€è¿æ¥æ± å¯¹è±¡
_db_pool = None

def init_db_pool():
    """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± """
    global _db_pool
    if not HAS_POOL:
        logger.info("è¿æ¥æ± åŠŸèƒ½æœªå¯ç”¨ï¼ˆDBUtilsæœªå®‰è£…ï¼‰")
        return

    if _db_pool is not None:
        logger.info("æ•°æ®åº“è¿æ¥æ± å·²å­˜åœ¨")
        return

    try:
        db_config = get_db_config()
        _db_pool = PooledDB(
            creator=pymysql,
            maxconnections=20,      # æœ€å¤§è¿æ¥æ•°
            mincached=2,            # æœ€å°ç©ºé—²è¿æ¥æ•°
            maxcached=10,           # æœ€å¤§ç©ºé—²è¿æ¥æ•°
            maxshared=0,            # æœ€å¤§å…±äº«è¿æ¥æ•°ï¼ˆ0è¡¨ç¤ºä¸å…±äº«ï¼‰
            blocking=True,          # è¿æ¥æ± æ»¡æ—¶æ˜¯å¦é˜»å¡ç­‰å¾…
            maxusage=None,          # å•ä¸ªè¿æ¥æœ€å¤§ä½¿ç”¨æ¬¡æ•°ï¼ˆNoneè¡¨ç¤ºæ— é™åˆ¶ï¼‰
            setsession=[],          # è¿æ¥å‰æ‰§è¡Œçš„SQLå‘½ä»¤
            ping=1,                 # ping MySQLæœåŠ¡ç«¯ï¼Œæ£€æŸ¥è¿æ¥æ˜¯å¦å¯ç”¨ï¼ˆ0=ä¸æ£€æŸ¥, 1=é»˜è®¤, 2=ä½¿ç”¨æ—¶æ£€æŸ¥, 4=äº‹åŠ¡å¼€å§‹æ—¶æ£€æŸ¥, 7=æ€»æ˜¯æ£€æŸ¥ï¼‰
            **db_config
        )
        logger.info(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆåŠŸ (æœ€å¤§è¿æ¥æ•°: 20, ç¼“å­˜è¿æ¥æ•°: 2-10)")
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
        _db_pool = None

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ï¼ˆä»è¿æ¥æ± æˆ–ç›´æ¥åˆ›å»ºï¼‰"""
    try:
        # å¦‚æœè¿æ¥æ± å¯ç”¨ï¼Œä»æ± ä¸­è·å–è¿æ¥
        if HAS_POOL and _db_pool is not None:
            return _db_pool.connection()
        # å¦åˆ™ç›´æ¥åˆ›å»ºè¿æ¥
        return pymysql.connect(**get_db_config())
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

# ==================== é…ç½®ç®¡ç†API ====================

@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–å½“å‰é…ç½®"""
    try:
        config = load_config()
        safe_config = {
            'database': {
                'host': config.get('database', {}).get('host', ''),
                'port': config.get('database', {}).get('port', 3306),
                'user': config.get('database', {}).get('user', ''),
                'password': '******' if config.get('database', {}).get('password') else '',
                'database': config.get('database', {}).get('database', ''),
                'charset': config.get('database', {}).get('charset', 'utf8mb4')
            },
            'app': config.get('app', DEFAULT_APP_CONFIG),
            'prometheus': config.get('prometheus', {}),
            'exporter_mapping': config.get('exporter_mapping', {}),
            'sqlserver_exporter_mapping': config.get('sqlserver_exporter_mapping', {})
        }
        return jsonify({'success': True, 'data': safe_config})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/config', methods=['POST'])
def update_config():
    """æ›´æ–°é…ç½®"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400

        config = load_config()

        if 'database' in data:
            db_data = data['database']
            for key in ['host', 'user', 'database', 'charset']:
                if key in db_data:
                    config.setdefault('database', {})[key] = db_data[key]
            if 'port' in db_data:
                config.setdefault('database', {})['port'] = int(db_data['port'])
            if 'password' in db_data and db_data['password'] != '******':
                config.setdefault('database', {})['password'] = db_data['password']

        if 'app' in data:
            config.setdefault('app', {}).update(data['app'])

        if save_config(config):
            return jsonify({'success': True, 'message': 'é…ç½®ä¿å­˜æˆåŠŸ'})
        return jsonify({'success': False, 'error': 'ä¿å­˜å¤±è´¥'}), 500

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/config/test', methods=['POST'])
def test_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400

        password = data.get('password', '')
        if password == '******':
            config = load_config()
            password = config.get('database', {}).get('password', '')

        test_config = {
            'host': data.get('host', '127.0.0.1'),
            'port': int(data.get('port', 3306)),
            'user': data.get('user', 'root'),
            'password': password,
            'database': data.get('database', 'db_monitor'),
            'charset': 'utf8mb4',
            'connect_timeout': 5,
            'cursorclass': pymysql.cursors.DictCursor
        }

        try:
            conn = pymysql.connect(**test_config)
            with conn.cursor() as cursor:
                cursor.execute("SELECT VERSION() as version")
                result = cursor.fetchone()
                db_version = result['version'] if result else 'Unknown'

                cursor.execute("""
                    SELECT COUNT(*) as count FROM information_schema.tables
                    WHERE table_schema = %s AND table_name IN ('db_instance_info', 'long_running_sql_log')
                """, (test_config['database'],))
                tables_exist = cursor.fetchone()['count'] >= 2
            conn.close()

            return jsonify({
                'success': True,
                'message': 'è¿æ¥æˆåŠŸ',
                'details': {'version': db_version, 'tables_initialized': tables_exist}
            })

        except pymysql.err.OperationalError as e:
            error_msgs = {
                1045: 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯',
                1049: 'æ•°æ®åº“ä¸å­˜åœ¨',
                2003: 'æ— æ³•è¿æ¥æœåŠ¡å™¨',
                2013: 'è¿æ¥è¶…æ—¶'
            }
            return jsonify({
                'success': False,
                'error': error_msgs.get(e.args[0], str(e)),
                'error_code': e.args[0]
            })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

def check_column_exists_func(cursor, table_name, column_name):
    """æ£€æŸ¥åˆ—æ˜¯å¦å­˜åœ¨"""
    cursor.execute(f"""
        SELECT COUNT(*) as count
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = DATABASE()
        AND TABLE_NAME = '{table_name}'
        AND COLUMN_NAME = '{column_name}'
    """)
    result = cursor.fetchone()
    return result[0] > 0

@app.route('/api/config/init-db', methods=['POST'])
def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        created_tables = []
        added_columns = []

        with conn.cursor() as cursor:
            # è¡¨0: æ•°æ®åº“ç‰ˆæœ¬è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS db_schema_version (
                    id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    version VARCHAR(20) NOT NULL COMMENT 'ç‰ˆæœ¬å·',
                    description TEXT COMMENT 'ç‰ˆæœ¬æè¿°',
                    upgrade_sql TEXT COMMENT 'å‡çº§SQLè„šæœ¬',
                    applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åº”ç”¨æ—¶é—´',
                    applied_by VARCHAR(50) DEFAULT 'system' COMMENT 'æ‰§è¡Œè€…',
                    INDEX idx_version (version),
                    INDEX idx_applied_at (applied_at)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ•°æ®åº“æ¶æ„ç‰ˆæœ¬è¡¨'
            """)
            created_tables.append('db_schema_version')
            # è¡¨1: æ•°æ®åº“å®ä¾‹ä¿¡æ¯è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS db_instance_info (
                    id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    db_project VARCHAR(100) NOT NULL COMMENT 'é¡¹ç›®åç§°',
                    db_ip VARCHAR(50) NOT NULL COMMENT 'æ•°æ®åº“IPåœ°å€',
                    db_port INT DEFAULT 3306 COMMENT 'æ•°æ®åº“ç«¯å£',
                    instance_name VARCHAR(100) COMMENT 'å®ä¾‹åç§°',
                    db_type VARCHAR(20) DEFAULT 'MySQL' COMMENT 'æ•°æ®åº“ç±»å‹',
                    db_user VARCHAR(50) COMMENT 'æ•°æ®åº“ç”¨æˆ·',
                    db_password VARCHAR(200) COMMENT 'æ•°æ®åº“å¯†ç ',
                    db_admin VARCHAR(50) COMMENT 'æ•°æ®åº“ç®¡ç†å‘˜',
                    db_version VARCHAR(50) COMMENT 'æ•°æ®åº“ç‰ˆæœ¬',
                    environment VARCHAR(20) DEFAULT 'production' COMMENT 'ç¯å¢ƒ',
                    status TINYINT DEFAULT 1 COMMENT 'çŠ¶æ€(1:å¯ç”¨ 0:ç¦ç”¨)',
                    description TEXT COMMENT 'æè¿°ä¿¡æ¯',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´',
                    INDEX idx_db_project (db_project),
                    INDEX idx_db_ip (db_ip),
                    INDEX idx_db_type (db_type),
                    INDEX idx_status (status)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ•°æ®åº“å®ä¾‹ä¿¡æ¯è¡¨'
            """)

            # è¡¨2: é•¿æ—¶é—´è¿è¡ŒSQLæ—¥å¿—è¡¨(å¢å¼ºç‰ˆ)
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS long_running_sql_log (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    db_instance_id INT NOT NULL COMMENT 'æ•°æ®åº“å®ä¾‹ID',
                    session_id VARCHAR(50) COMMENT 'ä¼šè¯ID',
                    serial_no VARCHAR(50) COMMENT 'åºåˆ—å·',
                    sql_id VARCHAR(100) COMMENT 'SQL ID',
                    sql_fingerprint VARCHAR(64) COMMENT 'SQLæŒ‡çº¹(MD5)',
                    sql_text VARCHAR(4000) COMMENT 'SQLæ–‡æœ¬(æˆªæ–­)',
                    sql_fulltext LONGTEXT COMMENT 'SQLå®Œæ•´æ–‡æœ¬',
                    username VARCHAR(100) COMMENT 'æ‰§è¡Œç”¨æˆ·',
                    machine VARCHAR(200) COMMENT 'å®¢æˆ·ç«¯æœºå™¨',
                    program VARCHAR(200) COMMENT 'å®¢æˆ·ç«¯ç¨‹åº',
                    module VARCHAR(200) COMMENT 'æ¨¡å—åç§°',
                    action VARCHAR(200) COMMENT 'æ“ä½œåç§°',
                    elapsed_seconds DECIMAL(15,2) DEFAULT 0 COMMENT 'è¿è¡Œç§’æ•°',
                    elapsed_minutes DECIMAL(15,4) DEFAULT 0 COMMENT 'è¿è¡Œåˆ†é’Ÿæ•°',
                    cpu_time DECIMAL(15,2) COMMENT 'CPUæ—¶é—´(ç§’)',
                    wait_time DECIMAL(15,2) COMMENT 'ç­‰å¾…æ—¶é—´(ç§’)',
                    logical_reads BIGINT COMMENT 'é€»è¾‘è¯»å–æ•°',
                    physical_reads BIGINT COMMENT 'ç‰©ç†è¯»å–æ•°',
                    rows_examined BIGINT COMMENT 'æ‰«æè¡Œæ•°',
                    rows_sent BIGINT COMMENT 'è¿”å›è¡Œæ•°',
                    query_cost DECIMAL(15,4) COMMENT 'æŸ¥è¯¢æˆæœ¬',
                    execution_plan JSON COMMENT 'æ‰§è¡Œè®¡åˆ’(JSONæ ¼å¼)',
                    index_used VARCHAR(500) COMMENT 'ä½¿ç”¨çš„ç´¢å¼•',
                    full_table_scan TINYINT DEFAULT 0 COMMENT 'æ˜¯å¦å…¨è¡¨æ‰«æ',
                    status VARCHAR(50) DEFAULT 'ACTIVE' COMMENT 'çŠ¶æ€',
                    blocking_session VARCHAR(50) COMMENT 'é˜»å¡ä¼šè¯ID',
                    wait_type VARCHAR(100) COMMENT 'ç­‰å¾…ç±»å‹',
                    wait_resource VARCHAR(200) COMMENT 'ç­‰å¾…èµ„æº',
                    event VARCHAR(200) COMMENT 'ç­‰å¾…äº‹ä»¶',
                    sql_exec_start DATETIME COMMENT 'SQLæ‰§è¡Œå¼€å§‹æ—¶é—´',
                    detect_time DATETIME NOT NULL COMMENT 'æ£€æµ‹æ—¶é—´',
                    alert_sent TINYINT DEFAULT 0 COMMENT 'æ˜¯å¦å·²å‘é€å‘Šè­¦',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'è®°å½•åˆ›å»ºæ—¶é—´',
                    INDEX idx_db_instance_id (db_instance_id),
                    INDEX idx_detect_time (detect_time),
                    INDEX idx_elapsed_minutes (elapsed_minutes),
                    INDEX idx_username (username),
                    INDEX idx_status (status),
                    INDEX idx_session_id (session_id),
                    INDEX idx_sql_fingerprint (sql_fingerprint),
                    INDEX idx_full_table_scan (full_table_scan),
                    INDEX idx_alert_sent (alert_sent)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='é•¿æ—¶é—´è¿è¡ŒSQLæ—¥å¿—è¡¨'
            """)

            # è¡¨3: æ­»é”ç›‘æ§æ—¥å¿—è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS deadlock_log (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    db_instance_id INT NOT NULL COMMENT 'æ•°æ®åº“å®ä¾‹ID',
                    deadlock_time DATETIME NOT NULL COMMENT 'æ­»é”å‘ç”Ÿæ—¶é—´',
                    victim_session_id VARCHAR(50) COMMENT 'å—å®³è€…ä¼šè¯ID',
                    victim_sql TEXT COMMENT 'å—å®³è€…SQL',
                    victim_trx_id VARCHAR(50) COMMENT 'å—å®³è€…äº‹åŠ¡ID',
                    blocker_session_id VARCHAR(50) COMMENT 'é˜»å¡è€…ä¼šè¯ID',
                    blocker_sql TEXT COMMENT 'é˜»å¡è€…SQL',
                    blocker_trx_id VARCHAR(50) COMMENT 'é˜»å¡è€…äº‹åŠ¡ID',
                    deadlock_graph JSON COMMENT 'å®Œæ•´æ­»é”å›¾(JSONæ ¼å¼)',
                    wait_resource VARCHAR(200) COMMENT 'ç­‰å¾…èµ„æº',
                    lock_mode VARCHAR(50) COMMENT 'é”æ¨¡å¼',
                    lock_type VARCHAR(50) COMMENT 'é”ç±»å‹',
                    isolation_level VARCHAR(50) COMMENT 'éš”ç¦»çº§åˆ«',
                    resolved_action VARCHAR(100) COMMENT 'è§£å†³åŠ¨ä½œ',
                    detect_time DATETIME DEFAULT CURRENT_TIMESTAMP COMMENT 'æ£€æµ‹æ—¶é—´',
                    alert_sent TINYINT DEFAULT 0 COMMENT 'æ˜¯å¦å·²å‘é€å‘Šè­¦',
                    alert_sent_time DATETIME COMMENT 'å‘Šè­¦å‘é€æ—¶é—´',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    INDEX idx_db_instance_id (db_instance_id),
                    INDEX idx_deadlock_time (deadlock_time),
                    INDEX idx_detect_time (detect_time),
                    INDEX idx_alert_sent (alert_sent)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='æ­»é”ç›‘æ§æ—¥å¿—è¡¨'
            """)

            # è¡¨4: ç›‘æ§å‘Šè­¦é…ç½®è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS monitor_alert_config (
                    id INT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    alert_name VARCHAR(100) NOT NULL COMMENT 'å‘Šè­¦åç§°',
                    alert_type VARCHAR(50) NOT NULL COMMENT 'å‘Šè­¦ç±»å‹',
                    threshold_warning DECIMAL(10,2) DEFAULT 5.0 COMMENT 'è­¦å‘Šé˜ˆå€¼(åˆ†é’Ÿ)',
                    threshold_critical DECIMAL(10,2) DEFAULT 10.0 COMMENT 'ä¸¥é‡é˜ˆå€¼(åˆ†é’Ÿ)',
                    notify_email VARCHAR(500) COMMENT 'é€šçŸ¥é‚®ç®±',
                    notify_webhook VARCHAR(500) COMMENT 'Webhooké€šçŸ¥åœ°å€',
                    is_enabled TINYINT DEFAULT 1 COMMENT 'æ˜¯å¦å¯ç”¨',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT 'æ›´æ–°æ—¶é—´'
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='ç›‘æ§å‘Šè­¦é…ç½®è¡¨'
            """)

            # è¡¨5: å‘Šè­¦å†å²è®°å½•è¡¨
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS alert_history (
                    id BIGINT AUTO_INCREMENT PRIMARY KEY COMMENT 'ä¸»é”®ID',
                    db_instance_id INT NOT NULL COMMENT 'æ•°æ®åº“å®ä¾‹ID',
                    sql_log_id BIGINT COMMENT 'å…³è”çš„SQLæ—¥å¿—ID',
                    alert_level VARCHAR(20) NOT NULL COMMENT 'å‘Šè­¦çº§åˆ«',
                    alert_message TEXT COMMENT 'å‘Šè­¦æ¶ˆæ¯',
                    is_acknowledged TINYINT DEFAULT 0 COMMENT 'æ˜¯å¦å·²ç¡®è®¤',
                    acknowledged_by VARCHAR(50) COMMENT 'ç¡®è®¤äºº',
                    acknowledged_at DATETIME COMMENT 'ç¡®è®¤æ—¶é—´',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT 'åˆ›å»ºæ—¶é—´',
                    INDEX idx_db_instance_id (db_instance_id),
                    INDEX idx_alert_level (alert_level),
                    INDEX idx_created_at (created_at)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='å‘Šè­¦å†å²è®°å½•è¡¨'
            """)

            # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„åˆ—
            # 1. long_running_sql_logè¡¨ç¼ºå¤±çš„å­—æ®µ
            if not check_column_exists_func(cursor, 'long_running_sql_log', 'wait_type'):
                cursor.execute("ALTER TABLE long_running_sql_log ADD COLUMN wait_type VARCHAR(100) COMMENT 'ç­‰å¾…ç±»å‹' AFTER blocking_session")
                added_columns.append('long_running_sql_log.wait_type')

            if not check_column_exists_func(cursor, 'long_running_sql_log', 'wait_resource'):
                cursor.execute("ALTER TABLE long_running_sql_log ADD COLUMN wait_resource VARCHAR(200) COMMENT 'ç­‰å¾…èµ„æº' AFTER wait_type")
                added_columns.append('long_running_sql_log.wait_resource')

            if not check_column_exists_func(cursor, 'long_running_sql_log', 'query_cost'):
                cursor.execute("ALTER TABLE long_running_sql_log ADD COLUMN query_cost DECIMAL(15,4) COMMENT 'æŸ¥è¯¢æˆæœ¬' AFTER rows_sent")
                added_columns.append('long_running_sql_log.query_cost')

            # 2. alert_historyè¡¨ç¼ºå¤±çš„å­—æ®µ
            if not check_column_exists_func(cursor, 'alert_history', 'alert_type'):
                cursor.execute("ALTER TABLE alert_history ADD COLUMN alert_type VARCHAR(50) NOT NULL DEFAULT 'unknown' COMMENT 'å‘Šè­¦ç±»å‹' AFTER alert_level")
                added_columns.append('alert_history.alert_type')

            if not check_column_exists_func(cursor, 'alert_history', 'alert_detail'):
                cursor.execute("ALTER TABLE alert_history ADD COLUMN alert_detail JSON COMMENT 'å‘Šè­¦è¯¦æƒ…(JSONæ ¼å¼)' AFTER alert_message")
                added_columns.append('alert_history.alert_detail')

            # æ’å…¥é»˜è®¤å‘Šè­¦é…ç½®
            cursor.execute("""
                INSERT IGNORE INTO monitor_alert_config
                (id, alert_name, alert_type, threshold_warning, threshold_critical, is_enabled)
                VALUES
                (1, 'é•¿æ—¶é—´SQLå‘Šè­¦', 'long_running_sql', 5.0, 10.0, 1),
                (2, 'ä¼šè¯é˜»å¡å‘Šè­¦', 'session_blocking', 3.0, 5.0, 1),
                (3, 'æ­»é”å‘Šè­¦', 'deadlock', 0.0, 0.0, 1),
                (4, 'è¿æ¥æ•°å‘Šè­¦', 'connection_usage', 80.0, 90.0, 1),
                (5, 'ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦', 'cache_hit_rate', 95.0, 90.0, 1),
                (6, 'å¤åˆ¶å»¶è¿Ÿå‘Šè­¦', 'replication_lag', 10.0, 60.0, 1)
            """)

            # è®°å½•ç‰ˆæœ¬
            cursor.execute("SELECT COUNT(*) FROM db_schema_version WHERE version = '1.2.0'")
            if cursor.fetchone()[0] == 0:
                cursor.execute("""
                    INSERT INTO db_schema_version (version, description)
                    VALUES ('1.2.0', 'å®Œå–„æ•°æ®åº“åˆå§‹åŒ–ï¼Œæ·»åŠ ç‰ˆæœ¬ç®¡ç†å’Œç¼ºå¤±å­—æ®µæ£€æŸ¥')
                """)

        conn.commit()
        conn.close()

        # æ„å»ºæˆåŠŸæ¶ˆæ¯
        message = 'æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸï¼'
        if created_tables:
            message += f' åˆ›å»ºäº† {len(created_tables)} ä¸ªè¡¨'
        if added_columns:
            message += f'ï¼Œæ·»åŠ äº† {len(added_columns)} ä¸ªç¼ºå¤±å­—æ®µ'

        return jsonify({
            'success': True,
            'message': message,
            'details': {
                'created_tables': created_tables,
                'added_columns': added_columns
            }
        })

    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== å‘Šè­¦é…ç½®API ====================

@app.route('/api/alert-config', methods=['GET'])
def get_alert_config():
    """è·å–å‘Šè­¦é…ç½®"""
    try:
        alert_config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'alert_config.json')

        if os.path.exists(alert_config_file):
            with open(alert_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
        else:
            # è¿”å›é»˜è®¤é…ç½®
            config = {
                'wecom': {'webhook': '', 'enabled': False},
                'dingtalk': {'webhook': '', 'secret': '', 'enabled': False},
                'email': {
                    'host': '', 'port': 465, 'user': '', 'password': '',
                    'from': '', 'to': [], 'enabled': False
                },
                'alert_rules': {
                    'slow_sql_threshold_minutes': 10,
                    'deadlock_always_alert': True,
                    'alert_interval_seconds': 300
                }
            }

        # éšè—æ•æ„Ÿä¿¡æ¯
        if config.get('wecom', {}).get('webhook'):
            webhook = config['wecom']['webhook']
            if len(webhook) > 20:
                config['wecom']['webhook_display'] = webhook[:20] + '...' + webhook[-10:]
            else:
                config['wecom']['webhook_display'] = webhook

        if config.get('email', {}).get('password'):
            config['email']['password'] = '******'

        return jsonify({'success': True, 'data': config})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/alert-config', methods=['POST'])
def update_alert_config():
    """æ›´æ–°å‘Šè­¦é…ç½®"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400

        alert_config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'alert_config.json')

        # è¯»å–ç°æœ‰é…ç½®
        if os.path.exists(alert_config_file):
            with open(alert_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
        else:
            config = {}

        # æ›´æ–°ä¼ä¸šå¾®ä¿¡é…ç½®
        if 'wecom' in data:
            wecom = data['wecom']
            config.setdefault('wecom', {})
            if 'webhook' in wecom and wecom['webhook']:
                config['wecom']['webhook'] = wecom['webhook']
            if 'enabled' in wecom:
                config['wecom']['enabled'] = wecom['enabled']

        # æ›´æ–°é’‰é’‰é…ç½®
        if 'dingtalk' in data:
            dingtalk = data['dingtalk']
            config.setdefault('dingtalk', {})
            if 'webhook' in dingtalk:
                config['dingtalk']['webhook'] = dingtalk['webhook']
            if 'secret' in dingtalk:
                config['dingtalk']['secret'] = dingtalk['secret']
            if 'enabled' in dingtalk:
                config['dingtalk']['enabled'] = dingtalk['enabled']

        # æ›´æ–°é‚®ä»¶é…ç½®
        if 'email' in data:
            email = data['email']
            config.setdefault('email', {})
            for key in ['host', 'port', 'user', 'from']:
                if key in email:
                    config['email'][key] = email[key]
            if 'password' in email and email['password'] != '******':
                config['email']['password'] = email['password']
            if 'to' in email:
                config['email']['to'] = email['to']
            if 'enabled' in email:
                config['email']['enabled'] = email['enabled']

        # æ›´æ–°å‘Šè­¦è§„åˆ™
        if 'alert_rules' in data:
            config.setdefault('alert_rules', {})
            config['alert_rules'].update(data['alert_rules'])

        # ä¿å­˜é…ç½®
        with open(alert_config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        return jsonify({'success': True, 'message': 'å‘Šè­¦é…ç½®ä¿å­˜æˆåŠŸ'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/alert-config/test', methods=['POST'])
def test_alert():
    """æµ‹è¯•å‘Šè­¦"""
    try:
        data = request.get_json()
        channel = data.get('channel', 'wecom')  # wecom, dingtalk, email

        alert_config_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'alert_config.json')

        if not os.path.exists(alert_config_file):
            return jsonify({'success': False, 'error': 'å‘Šè­¦é…ç½®æ–‡ä»¶ä¸å­˜åœ¨'}), 400

        with open(alert_config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # åŠ¨æ€å¯¼å…¥å‘Šè­¦æ¨¡å—
        import sys
        sys.path.insert(0, os.path.join(os.path.dirname(os.path.abspath(__file__)), 'utils'))
        from alert import WeComAlert, DingTalkAlert, EmailAlert

        test_title = "ğŸ§ª æ•°æ®åº“ç›‘æ§ç³»ç»Ÿæµ‹è¯•"
        test_content = f"è¿™æ˜¯ä¸€æ¡{channel}å‘Šè­¦æµ‹è¯•æ¶ˆæ¯ã€‚\n\næ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nå¦‚æœæ‚¨æ”¶åˆ°æ­¤æ¶ˆæ¯ï¼Œè¯´æ˜å‘Šè­¦é…ç½®æˆåŠŸï¼"

        if channel == 'wecom':
            webhook = config.get('wecom', {}).get('webhook', '')
            if not webhook:
                return jsonify({'success': False, 'error': 'ä¼ä¸šå¾®ä¿¡Webhookæœªé…ç½®'}), 400

            alert = WeComAlert(webhook)
            success = alert.send(test_title, test_content, 'INFO')

        elif channel == 'dingtalk':
            webhook = config.get('dingtalk', {}).get('webhook', '')
            secret = config.get('dingtalk', {}).get('secret', '')
            if not webhook:
                return jsonify({'success': False, 'error': 'é’‰é’‰Webhookæœªé…ç½®'}), 400

            alert = DingTalkAlert(webhook, secret)
            success = alert.send(test_title, test_content, 'INFO')

        elif channel == 'email':
            email_config = config.get('email', {})
            if not email_config.get('host'):
                return jsonify({'success': False, 'error': 'é‚®ä»¶é…ç½®ä¸å®Œæ•´'}), 400

            alert = EmailAlert(email_config)
            success = alert.send(test_title, test_content, 'INFO')

        else:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„å‘Šè­¦æ¸ é“'}), 400

        if success:
            return jsonify({'success': True, 'message': f'{channel}æµ‹è¯•æ¶ˆæ¯å‘é€æˆåŠŸ'})
        else:
            return jsonify({'success': False, 'error': 'æµ‹è¯•æ¶ˆæ¯å‘é€å¤±è´¥'}), 500

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== é‡‡é›†å™¨é…ç½®ç®¡ç†API ====================

@app.route('/api/collectors/config', methods=['GET'])
def get_collectors_config():
    """è·å–é‡‡é›†å™¨é…ç½®"""
    try:
        config = load_config()
        collectors_config = config.get('collectors', {
            'mysql': {
                'enabled': True,
                'interval': 60,
                'threshold': 5,
                'description': 'MySQL Performance Schemaé‡‡é›†å™¨'
            },
            'sqlserver': {
                'enabled': True,
                'interval': 60,
                'threshold': 5,
                'auto_enable_querystore': False,
                'description': 'SQL Server Query Storeé‡‡é›†å™¨'
            }
        })

        # è·å–é‡‡é›†å™¨çŠ¶æ€
        mysql_job = scheduler.get_job('mysql_collector')
        sqlserver_job = scheduler.get_job('sqlserver_collector')

        collectors_config['mysql']['status'] = 'running' if mysql_job else 'stopped'
        collectors_config['sqlserver']['status'] = 'running' if sqlserver_job else 'stopped'

        # è·å–ä¸‹æ¬¡è¿è¡Œæ—¶é—´
        if mysql_job and mysql_job.next_run_time:
            collectors_config['mysql']['next_run'] = mysql_job.next_run_time.strftime('%Y-%m-%d %H:%M:%S')
        if sqlserver_job and sqlserver_job.next_run_time:
            collectors_config['sqlserver']['next_run'] = sqlserver_job.next_run_time.strftime('%Y-%m-%d %H:%M:%S')

        return jsonify({'success': True, 'data': collectors_config})
    except Exception as e:
        logger.error(f"è·å–é‡‡é›†å™¨é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/collectors/config', methods=['POST'])
def update_collectors_config():
    """æ›´æ–°é‡‡é›†å™¨é…ç½®"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„è¯·æ±‚æ•°æ®'}), 400

        config = load_config()
        if 'collectors' not in config:
            config['collectors'] = {}

        # æ›´æ–°é…ç½®
        for collector_type in ['mysql', 'sqlserver']:
            if collector_type in data:
                collector_data = data[collector_type]
                if collector_type not in config['collectors']:
                    config['collectors'][collector_type] = {}

                # æ›´æ–°é…ç½®å­—æ®µ
                for key in ['enabled', 'interval', 'threshold', 'auto_enable_querystore', 'description']:
                    if key in collector_data:
                        config['collectors'][collector_type][key] = collector_data[key]

        # ä¿å­˜é…ç½®
        if not save_config(config):
            return jsonify({'success': False, 'error': 'ä¿å­˜é…ç½®å¤±è´¥'}), 500

        # å®æ—¶æ›´æ–°è°ƒåº¦å™¨
        for collector_type in ['mysql', 'sqlserver']:
            if collector_type in data:
                collector_config = config['collectors'][collector_type]
                update_collector_schedule(
                    collector_type,
                    collector_config.get('enabled', True),
                    collector_config.get('interval', 60)
                )

        return jsonify({'success': True, 'message': 'é‡‡é›†å™¨é…ç½®å·²æ›´æ–°å¹¶å®æ—¶ç”Ÿæ•ˆ'})

    except Exception as e:
        logger.error(f"æ›´æ–°é‡‡é›†å™¨é…ç½®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/collectors/status', methods=['GET'])
def get_collectors_status():
    """è·å–é‡‡é›†å™¨è¿è¡ŒçŠ¶æ€"""
    try:
        status = {}

        for collector_type in ['mysql', 'sqlserver']:
            job_id = f"{collector_type}_collector"
            job = scheduler.get_job(job_id)

            if job:
                status[collector_type] = {
                    'running': True,
                    'next_run': job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else None,
                    'trigger': str(job.trigger)
                }
            else:
                status[collector_type] = {
                    'running': False,
                    'next_run': None,
                    'trigger': None
                }

        return jsonify({'success': True, 'data': status})
    except Exception as e:
        logger.error(f"è·å–é‡‡é›†å™¨çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/collectors/<collector_type>/trigger', methods=['POST'])
def trigger_collector(collector_type):
    """æ‰‹åŠ¨è§¦å‘é‡‡é›†å™¨æ‰§è¡Œä¸€æ¬¡"""
    try:
        if collector_type not in ['mysql', 'sqlserver']:
            return jsonify({'success': False, 'error': 'æ— æ•ˆçš„é‡‡é›†å™¨ç±»å‹'}), 400

        # ç«‹å³æ‰§è¡Œä¸€æ¬¡
        if collector_type == 'mysql':
            run_mysql_collector()
        else:
            run_sqlserver_collector()

        return jsonify({'success': True, 'message': f'{collector_type}é‡‡é›†å™¨å·²æ‰‹åŠ¨è§¦å‘æ‰§è¡Œ'})
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨è§¦å‘é‡‡é›†å™¨å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== å®ä¾‹ç®¡ç†API ====================

@app.route('/api/instances', methods=['GET'])
def get_instances():
    """è·å–å®ä¾‹åˆ—è¡¨"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT id, db_project, db_ip, db_port, instance_name, db_type,
                       db_user, db_admin, environment, status, description,
                       created_at, updated_at
                FROM db_instance_info ORDER BY db_project, db_ip
            """)
            results = cursor.fetchall()
            for row in results:
                for k, v in row.items():
                    if isinstance(v, datetime):
                        row[k] = v.strftime('%Y-%m-%d %H:%M:%S')
        conn.close()
        return jsonify({'success': True, 'data': results, 'total': len(results)})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/instances', methods=['POST'])
def add_instance():
    """æ·»åŠ å®ä¾‹"""
    try:
        data = request.get_json()
        if not data or not data.get('db_project') or not data.get('db_ip'):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…å¡«å­—æ®µ'}), 400

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                INSERT INTO db_instance_info
                (db_project, db_ip, db_port, instance_name, db_type, db_user, db_password, db_admin, environment, description)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                data.get('db_project'), data.get('db_ip'), data.get('db_port', 3306),
                data.get('instance_name', ''), data.get('db_type', 'MySQL'),
                data.get('db_user', ''), data.get('db_password', ''),
                data.get('db_admin', ''), data.get('environment', 'production'),
                data.get('description', '')
            ))
            instance_id = cursor.lastrowid
        conn.commit()
        conn.close()
        return jsonify({'success': True, 'message': 'æ·»åŠ æˆåŠŸ', 'id': instance_id})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/instances/<int:id>', methods=['PUT'])
def update_instance(id):
    """æ›´æ–°å®ä¾‹"""
    logger.info(f"Update instance called with id={id}")
    try:
        data = request.get_json()
        logger.info(f"Request data: {data}")
        if not data:
            return jsonify({'success': False, 'error': 'æ— æ•ˆæ•°æ®'}), 400

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        fields = ['db_project', 'db_ip', 'db_port', 'instance_name', 'db_type',
                  'db_user', 'db_password', 'db_admin', 'environment', 'status', 'description']
        updates = []
        params = []
        for f in fields:
            if f in data:
                updates.append(f"{f} = %s")
                params.append(data[f])

        if not updates:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰æ›´æ–°å­—æ®µ'}), 400

        params.append(id)
        with conn.cursor() as cursor:
            cursor.execute(f"UPDATE db_instance_info SET {', '.join(updates)} WHERE id = %s", params)
            affected = cursor.rowcount
        conn.commit()
        conn.close()

        if affected > 0:
            return jsonify({'success': True, 'message': 'æ›´æ–°æˆåŠŸ'})
        return jsonify({'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}), 404

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/instances/<int:id>', methods=['DELETE'])
def delete_instance(id):
    """åˆ é™¤å®ä¾‹"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("DELETE FROM long_running_sql_log WHERE db_instance_id = %s", (id,))
            cursor.execute("DELETE FROM db_instance_info WHERE id = %s", (id,))
            affected = cursor.rowcount
        conn.commit()
        conn.close()

        if affected > 0:
            return jsonify({'success': True, 'message': 'åˆ é™¤æˆåŠŸ'})
        return jsonify({'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}), 404

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/instances/<int:id>/test', methods=['POST'])
def test_instance_connection(id):
    """æµ‹è¯•å®ä¾‹è¿æ¥"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç®¡ç†æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT db_ip, db_port, db_type, db_user, db_password, instance_name
                FROM db_instance_info WHERE id = %s
            """, (id,))
            instance = cursor.fetchone()
        conn.close()

        if not instance:
            return jsonify({'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}), 404

        db_type = instance['db_type'] or 'MySQL'

        # æ ¹æ®æ•°æ®åº“ç±»å‹æµ‹è¯•è¿æ¥
        if db_type == 'MySQL':
            try:
                test_conn = pymysql.connect(
                    host=instance['db_ip'],
                    port=instance['db_port'] or 3306,
                    user=instance['db_user'] or 'root',
                    password=instance['db_password'] or '',
                    connect_timeout=5,
                    cursorclass=pymysql.cursors.DictCursor
                )
                with test_conn.cursor() as cursor:
                    cursor.execute("SELECT VERSION() as version")
                    result = cursor.fetchone()
                    version = result['version'] if result else 'Unknown'
                test_conn.close()
                return jsonify({
                    'success': True,
                    'message': 'MySQLè¿æ¥æˆåŠŸ',
                    'details': {'version': version, 'type': 'MySQL'}
                })
            except pymysql.err.OperationalError as e:
                error_msgs = {
                    1045: 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯',
                    2003: 'æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨',
                    2013: 'è¿æ¥è¶…æ—¶'
                }
                return jsonify({
                    'success': False,
                    'error': error_msgs.get(e.args[0], str(e))
                })
        elif db_type == 'SQLServer':
            try:
                import pyodbc
                # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
                conn_str = (
                    f"DRIVER={{ODBC Driver 18 for SQL Server}};"
                    f"SERVER={instance['db_ip']},{instance['db_port'] or 1433};"
                    f"UID={instance['db_user'] or 'sa'};"
                    f"PWD={instance['db_password'] or ''};"
                    f"Encrypt=no;"
                    f"TrustServerCertificate=yes;"
                )
                test_conn = pyodbc.connect(conn_str, timeout=5)
                cursor = test_conn.cursor()
                cursor.execute("SELECT @@VERSION as version")
                row = cursor.fetchone()
                version = row.version.split('\n')[0] if row else 'Unknown'
                cursor.close()
                test_conn.close()
                return jsonify({
                    'success': True,
                    'message': 'SQL Serverè¿æ¥æˆåŠŸ',
                    'details': {'version': version, 'type': 'SQLServer'}
                })
            except pyodbc.Error as e:
                error_msg = str(e)
                if 'Login failed' in error_msg:
                    error_msg = 'ç”¨æˆ·åæˆ–å¯†ç é”™è¯¯'
                elif 'Cannot open' in error_msg or 'timeout' in error_msg.lower():
                    error_msg = 'æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨æˆ–è¿æ¥è¶…æ—¶'
                return jsonify({
                    'success': False,
                    'error': f'SQL Serverè¿æ¥å¤±è´¥: {error_msg}'
                })
            except ImportError:
                return jsonify({
                    'success': False,
                    'error': 'pyodbcæ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install pyodbc'
                })
        elif db_type in ['Oracle', 'PostgreSQL']:
            return jsonify({
                'success': False,
                'error': f'{db_type} è¿æ¥æµ‹è¯•æš‚æœªå®ç°ï¼Œè¯·å…ˆé…ç½®ç›¸åº”çš„æ•°æ®åº“é©±åŠ¨'
            })
        else:
            return jsonify({'success': False, 'error': 'ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== ç›‘æ§æ•°æ®API ====================

@app.route('/')
def index():
    try:
        return send_from_directory('static', 'index.html')
    except:
        return '<h1>è¯·å°†index.htmlæ”¾åˆ°staticç›®å½•</h1>'

@app.route('/api/health', methods=['GET'])
def health_check():
    db_status = 'connected'
    try:
        conn = get_db_connection()
        if conn:
            conn.close()
        else:
            db_status = 'disconnected'
    except:
        db_status = 'error'

    return jsonify({
        'status': 'healthy',
        'database': db_status,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'version': '1.1.0'
    })

@app.route('/api/long_sql', methods=['GET'])
def get_long_running_sql():
    try:
        hours = request.args.get('hours', 24, type=int)
        instance_id = request.args.get('instance_id', type=int)
        min_minutes = request.args.get('min_minutes', 0.0, type=float)
        page = max(1, request.args.get('page', 1, type=int))
        page_size = min(100, max(1, request.args.get('page_size', 20, type=int)))
        offset = (page - 1) * page_size

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            where = ["l.detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)", "l.elapsed_minutes >= %s"]
            params = [hours, min_minutes]

            if instance_id:
                where.append("l.db_instance_id = %s")
                params.append(instance_id)

            where_clause = " AND ".join(where)

            cursor.execute(f"SELECT COUNT(*) as cnt FROM long_running_sql_log l WHERE {where_clause}", params)
            total = cursor.fetchone()['cnt']
            total_pages = math.ceil(total / page_size) if page_size > 0 else 1

            cursor.execute(f"""
                SELECT l.*, i.db_project, i.db_ip, i.db_port, i.instance_name
                FROM long_running_sql_log l
                LEFT JOIN db_instance_info i ON l.db_instance_id = i.id
                WHERE {where_clause}
                ORDER BY l.detect_time DESC LIMIT %s OFFSET %s
            """, params + [page_size, offset])
            results = cursor.fetchall()

            for row in results:
                for k, v in row.items():
                    if isinstance(v, datetime):
                        row[k] = v.strftime('%Y-%m-%d %H:%M:%S')
                    elif isinstance(v, timedelta):
                        row[k] = str(v)

        conn.close()
        return jsonify({
            'success': True, 'data': results,
            'pagination': {
                'current_page': page, 'page_size': page_size,
                'total_count': total, 'total_pages': total_pages,
                'has_prev': page > 1, 'has_next': page < total_pages
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/long_sql/<int:sql_id>', methods=['GET'])
def get_long_sql_detail(sql_id):
    """è·å–å•ä¸ªæ…¢SQLè¯¦ç»†ä¿¡æ¯"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT l.*, i.db_project, i.db_ip, i.db_port, i.instance_name, i.db_type
                FROM long_running_sql_log l
                LEFT JOIN db_instance_info i ON l.db_instance_id = i.id
                WHERE l.id = %s
            """, (sql_id,))
            result = cursor.fetchone()

            if not result:
                return jsonify({'success': False, 'error': 'SQLè®°å½•ä¸å­˜åœ¨'}), 404

            # è½¬æ¢æ—¥æœŸæ—¶é—´æ ¼å¼
            for k, v in result.items():
                if isinstance(v, datetime):
                    result[k] = v.strftime('%Y-%m-%d %H:%M:%S')
                elif isinstance(v, timedelta):
                    result[k] = str(v)

        conn.close()
        return jsonify({'success': True, 'data': result})

    except Exception as e:
        logger.error(f"è·å–SQLè¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/deadlocks', methods=['GET'])
def get_deadlocks():
    """è·å–æ­»é”åˆ—è¡¨"""
    try:
        hours = request.args.get('hours', 24, type=int)
        instance_id = request.args.get('instance_id', type=int)
        page = max(1, request.args.get('page', 1, type=int))
        page_size = min(100, max(1, request.args.get('page_size', 20, type=int)))
        offset = (page - 1) * page_size

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            where = ["d.detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)"]
            params = [hours]

            if instance_id:
                where.append("d.db_instance_id = %s")
                params.append(instance_id)

            where_clause = " AND ".join(where)

            cursor.execute(f"SELECT COUNT(*) as cnt FROM deadlock_log d WHERE {where_clause}", params)
            total = cursor.fetchone()['cnt']
            total_pages = math.ceil(total / page_size) if page_size > 0 else 1

            cursor.execute(f"""
                SELECT d.*, i.db_project, i.db_ip, i.db_port, i.instance_name, i.db_type
                FROM deadlock_log d
                LEFT JOIN db_instance_info i ON d.db_instance_id = i.id
                WHERE {where_clause}
                ORDER BY d.detect_time DESC LIMIT %s OFFSET %s
            """, params + [page_size, offset])
            results = cursor.fetchall()

            for row in results:
                for k, v in row.items():
                    if isinstance(v, datetime):
                        row[k] = v.strftime('%Y-%m-%d %H:%M:%S')

        conn.close()
        return jsonify({
            'success': True, 'data': results,
            'pagination': {
                'current_page': page, 'page_size': page_size,
                'total_count': total, 'total_pages': total_pages,
                'has_prev': page > 1, 'has_next': page < total_pages
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/statistics', methods=['GET'])
def get_statistics():
    try:
        hours = request.args.get('hours', 24, type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # ç»Ÿè®¡å¯ç”¨çš„å®ä¾‹æ€»æ•°
            cursor.execute("SELECT COUNT(*) as total_instances FROM db_instance_info WHERE status = 1")
            instance_stat = cursor.fetchone()
            total_instances = instance_stat.get('total_instances', 0) if instance_stat else 0

            # æ­»é”ç»Ÿè®¡
            cursor.execute("""
                SELECT COUNT(*) as deadlock_count
                FROM deadlock_log WHERE detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)
            """, (hours,))
            deadlock_stat = cursor.fetchone()

            # æ…¢SQLç»Ÿè®¡
            cursor.execute("""
                SELECT COUNT(*) as total_sql_count, AVG(elapsed_minutes) as avg_duration,
                       MAX(elapsed_minutes) as max_duration,
                       SUM(CASE WHEN elapsed_minutes > 10 THEN 1 ELSE 0 END) as critical_count,
                       SUM(CASE WHEN elapsed_minutes > 5 AND elapsed_minutes <= 10 THEN 1 ELSE 0 END) as warning_count,
                       SUM(CASE WHEN elapsed_minutes <= 5 THEN 1 ELSE 0 END) as normal_count
                FROM long_running_sql_log WHERE detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)
            """, (hours,))
            summary = cursor.fetchone()

            # æ·»åŠ å®ä¾‹æ€»æ•°åˆ°summary
            if summary:
                summary['instance_count'] = total_instances

            cursor.execute("""
                SELECT i.db_project, i.db_ip, i.instance_name, COUNT(l.id) as sql_count,
                       AVG(l.elapsed_minutes) as avg_duration, MAX(l.elapsed_minutes) as max_duration
                FROM db_instance_info i
                LEFT JOIN long_running_sql_log l ON i.id = l.db_instance_id
                    AND l.detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)
                GROUP BY i.id ORDER BY sql_count DESC
            """, (hours,))
            instances = cursor.fetchall()

            cursor.execute("""
                SELECT DATE_FORMAT(detect_time, '%%Y-%%m-%%d %%H:00') as hour_time,
                       COUNT(*) as sql_count, AVG(elapsed_minutes) as avg_duration
                FROM long_running_sql_log WHERE detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)
                GROUP BY hour_time ORDER BY hour_time
            """, (hours,))
            trend = cursor.fetchall()

        conn.close()

        # åˆå¹¶æ­»é”ç»Ÿè®¡åˆ°summary
        summary_with_deadlock = summary or {}
        summary_with_deadlock['deadlock_count'] = deadlock_stat.get('deadlock_count', 0) if deadlock_stat else 0

        return jsonify({
            'success': True,
            'summary': summary_with_deadlock,
            'instances': instances or [],
            'trend': trend or []
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/performance_metrics', methods=['GET'])
def get_performance_metrics():
    """è·å–æ€§èƒ½æŒ‡æ ‡ï¼šQPSã€TPSã€è¿æ¥æ•°ã€ç¼“å­˜å‘½ä¸­ç‡ç­‰"""
    try:
        instance_id = request.args.get('instance_id', type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–å®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE status = 1 LIMIT 10
                """)
                instances = cursor.fetchall()

        conn.close()

        metrics_list = []
        for instance in instances:
            if not instance or instance['db_type'] != 'MySQL':
                continue

            try:
                # è¿æ¥ç›®æ ‡MySQLå®ä¾‹
                target_conn = pymysql.connect(
                    host=instance['db_ip'],
                    port=instance['db_port'],
                    user=instance['db_user'],
                    password=instance['db_password'],
                    connect_timeout=3,
                    cursorclass=pymysql.cursors.DictCursor
                )

                with target_conn.cursor() as cursor:
                    metrics = {
                        'instance_id': instance['id'],
                        'db_project': instance['db_project'],
                        'db_ip': instance['db_ip'],
                        'db_port': instance['db_port'],
                        'instance_name': instance['instance_name'] or f"{instance['db_ip']}:{instance['db_port']}"
                    }

                    # QPS/TPSè®¡ç®—
                    cursor.execute("SHOW GLOBAL STATUS WHERE variable_name IN ('Questions', 'Com_commit', 'Com_rollback', 'Uptime')")
                    status_vars = {row['Variable_name']: int(row['Value']) for row in cursor.fetchall()}

                    uptime = status_vars.get('Uptime', 1)
                    if uptime > 0:
                        metrics['qps'] = round(status_vars.get('Questions', 0) / uptime, 2)
                        metrics['tps'] = round((status_vars.get('Com_commit', 0) + status_vars.get('Com_rollback', 0)) / uptime, 2)
                    else:
                        metrics['qps'] = 0
                        metrics['tps'] = 0

                    # è¿æ¥æ•°ç»Ÿè®¡
                    cursor.execute("SHOW GLOBAL STATUS LIKE 'Threads_connected'")
                    threads_result = cursor.fetchone()
                    current_connections = int(threads_result['Value']) if threads_result else 0

                    cursor.execute("SHOW VARIABLES LIKE 'max_connections'")
                    max_result = cursor.fetchone()
                    max_connections = int(max_result['Value']) if max_result else 1

                    metrics['current_connections'] = current_connections
                    metrics['max_connections'] = max_connections
                    metrics['connection_usage'] = round(current_connections / max_connections * 100, 2)
                    metrics['connection_warning'] = metrics['connection_usage'] > 80  # >80%å‘Šè­¦

                    # ç¼“å­˜å‘½ä¸­ç‡
                    cursor.execute("SHOW GLOBAL STATUS LIKE 'Innodb_buffer_pool_read%'")
                    cache_stats = {row['Variable_name']: int(row['Value']) for row in cursor.fetchall()}

                    read_requests = cache_stats.get('Innodb_buffer_pool_read_requests', 0)
                    read_disk = cache_stats.get('Innodb_buffer_pool_reads', 0)

                    if read_requests + read_disk > 0:
                        metrics['cache_hit_rate'] = round(read_requests / (read_requests + read_disk) * 100, 2)
                        metrics['cache_warning'] = metrics['cache_hit_rate'] < 95  # <95%å‘Šè­¦
                    else:
                        metrics['cache_hit_rate'] = 100
                        metrics['cache_warning'] = False

                    # æ…¢æŸ¥è¯¢ç»Ÿè®¡
                    cursor.execute("SHOW GLOBAL STATUS LIKE 'Slow_queries'")
                    slow_result = cursor.fetchone()
                    metrics['slow_queries'] = int(slow_result['Value']) if slow_result else 0

                    metrics_list.append(metrics)

                target_conn.close()

            except Exception as e:
                logger.error(f"è·å–å®ä¾‹{instance['db_project']}æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
                continue

        return jsonify({'success': True, 'data': metrics_list})

    except Exception as e:
        logger.error(f"è·å–æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/sqlserver/performance_metrics', methods=['GET'])
def get_sqlserver_performance_metrics():
    """è·å–SQL Serveræ€§èƒ½æŒ‡æ ‡ï¼šBatch Requests/secã€TPSã€è¿æ¥æ•°ã€ç¼“å­˜å‘½ä¸­ç‡ç­‰"""
    try:
        instance_id = request.args.get('instance_id', type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–SQL Serverå®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE status = 1 AND db_type = 'SQL Server' LIMIT 10
                """)
                instances = cursor.fetchall()

        conn.close()

        metrics_list = []
        for instance in instances:
            if not instance or instance['db_type'] != 'SQL Server':
                continue

            try:
                # è¿æ¥ç›®æ ‡SQL Serverå®ä¾‹
                conn_str = (
                    f"DRIVER={{ODBC Driver 17 for SQL Server}};"
                    f"SERVER={instance['db_ip']},{instance['db_port']};"
                    f"UID={instance['db_user']};"
                    f"PWD={instance['db_password']};"
                    f"Timeout=3;"
                )
                target_conn = pyodbc.connect(conn_str)
                cursor = target_conn.cursor()

                metrics = {
                    'instance_id': instance['id'],
                    'db_project': instance['db_project'],
                    'db_ip': instance['db_ip'],
                    'db_port': instance['db_port'],
                    'instance_name': instance['instance_name'] or f"{instance['db_ip']}:{instance['db_port']}"
                }

                # è·å–æ€§èƒ½è®¡æ•°å™¨
                perf_query = """
                SELECT
                    counter_name,
                    cntr_value,
                    cntr_type
                FROM sys.dm_os_performance_counters
                WHERE (object_name LIKE '%General Statistics%' AND counter_name = 'User Connections')
                   OR (object_name LIKE '%SQL Statistics%' AND counter_name IN ('Batch Requests/sec', 'SQL Compilations/sec'))
                   OR (object_name LIKE '%Buffer Manager%' AND counter_name IN ('Buffer cache hit ratio', 'Page life expectancy', 'Buffer cache hit ratio base'))
                   OR (object_name LIKE '%Databases%' AND counter_name = 'Transactions/sec' AND instance_name = '_Total')
                """
                cursor.execute(perf_query)
                perf_counters = {row.counter_name: row.cntr_value for row in cursor.fetchall()}

                # è¿æ¥æ•°
                metrics['current_connections'] = int(perf_counters.get('User Connections', 0))

                # è·å–æœ€å¤§è¿æ¥æ•°
                cursor.execute("SELECT @@MAX_CONNECTIONS as max_conn")
                max_conn_row = cursor.fetchone()
                max_connections = max_conn_row.max_conn if max_conn_row else 32767
                metrics['max_connections'] = max_connections
                metrics['connection_usage'] = round(metrics['current_connections'] / max_connections * 100, 2) if max_connections > 0 else 0
                metrics['connection_warning'] = metrics['connection_usage'] > 80

                # Batch Requests/sec (ç±»ä¼¼QPS)
                metrics['batch_requests_per_sec'] = round(float(perf_counters.get('Batch Requests/sec', 0)), 2)

                # TPS
                metrics['tps'] = round(float(perf_counters.get('Transactions/sec', 0)), 2)

                # SQLç¼–è¯‘æ¬¡æ•°/ç§’
                metrics['sql_compilations_per_sec'] = round(float(perf_counters.get('SQL Compilations/sec', 0)), 2)

                # Buffer Cache Hit Ratio (ç¼“å­˜å‘½ä¸­ç‡)
                buffer_hit = float(perf_counters.get('Buffer cache hit ratio', 0))
                buffer_hit_base = float(perf_counters.get('Buffer cache hit ratio base', 1))
                if buffer_hit_base > 0:
                    metrics['cache_hit_rate'] = round(buffer_hit / buffer_hit_base * 100, 2)
                else:
                    metrics['cache_hit_rate'] = 100
                metrics['cache_warning'] = metrics['cache_hit_rate'] < 90

                # Page Life Expectancy (é¡µé¢ç”Ÿå­˜æœŸï¼Œç§’)
                metrics['page_life_expectancy'] = int(perf_counters.get('Page life expectancy', 0))
                metrics['ple_warning'] = metrics['page_life_expectancy'] < 300  # <5åˆ†é’Ÿå‘Šè­¦

                # CPUä½¿ç”¨ç‡
                cpu_query = """
                SELECT TOP 1
                    SQLProcessUtilization as sql_cpu,
                    100 - SystemIdle as total_cpu
                FROM (
                    SELECT
                        record.value('(./Record/@id)[1]', 'int') AS record_id,
                        record.value('(./Record/SchedulerMonitorEvent/SystemHealth/SystemIdle)[1]', 'int') AS SystemIdle,
                        record.value('(./Record/SchedulerMonitorEvent/SystemHealth/ProcessUtilization)[1]', 'int') AS SQLProcessUtilization,
                        DATEADD(ms, -1 * ((SELECT ms_ticks FROM sys.dm_os_sys_info) - timestamp), GETDATE()) AS EventTime
                    FROM (
                        SELECT timestamp, CONVERT(xml, record) AS record
                        FROM sys.dm_os_ring_buffers
                        WHERE ring_buffer_type = N'RING_BUFFER_SCHEDULER_MONITOR'
                        AND record LIKE '%<SystemHealth>%'
                    ) AS x
                ) AS y
                ORDER BY record_id DESC
                """
                cursor.execute(cpu_query)
                cpu_row = cursor.fetchone()
                if cpu_row:
                    metrics['sql_cpu_percent'] = cpu_row.sql_cpu
                    metrics['total_cpu_percent'] = cpu_row.total_cpu
                    metrics['cpu_warning'] = cpu_row.sql_cpu > 80
                else:
                    metrics['sql_cpu_percent'] = 0
                    metrics['total_cpu_percent'] = 0
                    metrics['cpu_warning'] = False

                # å†…å­˜ä½¿ç”¨æƒ…å†µ
                memory_query = """
                SELECT
                    (total_physical_memory_kb / 1024) as total_memory_mb,
                    (available_physical_memory_kb / 1024) as available_memory_mb,
                    (total_physical_memory_kb - available_physical_memory_kb) * 100.0 / total_physical_memory_kb as memory_usage_percent
                FROM sys.dm_os_sys_memory
                """
                cursor.execute(memory_query)
                mem_row = cursor.fetchone()
                if mem_row:
                    metrics['total_memory_mb'] = int(mem_row.total_memory_mb)
                    metrics['available_memory_mb'] = int(mem_row.available_memory_mb)
                    metrics['memory_usage_percent'] = round(mem_row.memory_usage_percent, 2)
                    metrics['memory_warning'] = mem_row.memory_usage_percent > 90
                else:
                    metrics['total_memory_mb'] = 0
                    metrics['available_memory_mb'] = 0
                    metrics['memory_usage_percent'] = 0
                    metrics['memory_warning'] = False

                # ç­‰å¾…ç»Ÿè®¡ï¼ˆTop 5ï¼‰
                wait_query = """
                SELECT TOP 5
                    wait_type,
                    wait_time_ms / 1000.0 as wait_time_sec,
                    waiting_tasks_count
                FROM sys.dm_os_wait_stats
                WHERE wait_type NOT IN (
                    'CLR_SEMAPHORE', 'LAZYWRITER_SLEEP', 'RESOURCE_QUEUE', 'SLEEP_TASK',
                    'SLEEP_SYSTEMTASK', 'SQLTRACE_BUFFER_FLUSH', 'WAITFOR', 'LOGMGR_QUEUE',
                    'CHECKPOINT_QUEUE', 'REQUEST_FOR_DEADLOCK_SEARCH', 'XE_TIMER_EVENT',
                    'BROKER_TO_FLUSH', 'BROKER_TASK_STOP', 'CLR_MANUAL_EVENT',
                    'CLR_AUTO_EVENT', 'DISPATCHER_QUEUE_SEMAPHORE', 'FT_IFTS_SCHEDULER_IDLE_WAIT',
                    'XE_DISPATCHER_WAIT', 'XE_DISPATCHER_JOIN', 'SQLTRACE_INCREMENTAL_FLUSH_SLEEP'
                )
                ORDER BY wait_time_ms DESC
                """
                cursor.execute(wait_query)
                wait_stats = []
                for row in cursor.fetchall():
                    wait_stats.append({
                        'wait_type': row.wait_type,
                        'wait_time_sec': round(row.wait_time_sec, 2),
                        'waiting_tasks': row.waiting_tasks_count
                    })
                metrics['top_waits'] = wait_stats

                # é˜»å¡ä¼šè¯æ•°é‡
                blocking_query = """
                SELECT COUNT(DISTINCT blocked.session_id) as blocked_count
                FROM sys.dm_exec_requests blocked
                WHERE blocked.blocking_session_id > 0
                """
                cursor.execute(blocking_query)
                blocked_row = cursor.fetchone()
                metrics['blocked_sessions'] = blocked_row.blocked_count if blocked_row else 0
                metrics['blocking_warning'] = metrics['blocked_sessions'] > 0

                metrics_list.append(metrics)

                cursor.close()
                target_conn.close()

            except Exception as e:
                logger.error(f"è·å–SQL Serverå®ä¾‹{instance['db_project']}æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
                continue

        return jsonify({'success': True, 'data': metrics_list})

    except Exception as e:
        logger.error(f"è·å–SQL Serveræ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/blocking_queries', methods=['GET'])
def get_blocking_queries():
    """è·å–é˜»å¡æŸ¥è¯¢ï¼ˆä½¿ç”¨sys.innodb_lock_waitsï¼‰"""
    try:
        instance_id = request.args.get('instance_id', type=int)
        min_wait_seconds = request.args.get('min_wait_seconds', 10, type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–å®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE status = 1
                """)
                instances = cursor.fetchall()

        conn.close()

        blocking_list = []
        for instance in instances:
            if not instance or instance['db_type'] != 'MySQL':
                continue

            try:
                target_conn = pymysql.connect(
                    host=instance['db_ip'],
                    port=instance['db_port'],
                    user=instance['db_user'],
                    password=instance['db_password'],
                    connect_timeout=3,
                    cursorclass=pymysql.cursors.DictCursor
                )

                with target_conn.cursor() as cursor:
                    # æ£€æŸ¥MySQLç‰ˆæœ¬ï¼Œä½¿ç”¨sys.innodb_lock_waits (MySQL 5.7+)
                    cursor.execute("SELECT VERSION() as version")
                    version_result = cursor.fetchone()
                    version = version_result['version'] if version_result else ''

                    # å°è¯•ä½¿ç”¨sys schema (MySQL 5.7+æ¨èæ–¹å¼)
                    try:
                        cursor.execute(f"""
                            SELECT
                                waiting_pid as blocked_thread,
                                waiting_query as blocked_sql,
                                blocking_pid as blocking_thread,
                                blocking_query as blocking_sql,
                                wait_age as wait_time,
                                sql_kill_blocking_query as kill_command
                            FROM sys.innodb_lock_waits
                            WHERE TIMESTAMPDIFF(SECOND, wait_started, NOW()) >= {min_wait_seconds}
                        """)
                        results = cursor.fetchall()

                        for row in results:
                            blocking_list.append({
                                'instance_id': instance['id'],
                                'db_project': instance['db_project'],
                                'db_ip': instance['db_ip'],
                                'db_port': instance['db_port'],
                                'blocked_thread': row['blocked_thread'],
                                'blocked_sql': row['blocked_sql'] or '',
                                'blocking_thread': row['blocking_thread'],
                                'blocking_sql': row['blocking_sql'] or '',
                                'wait_time': row['wait_time'],
                                'kill_command': row['kill_command'],
                                'detection_method': 'sys.innodb_lock_waits'
                            })

                    except Exception:
                        # å¦‚æœsys schemaä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ (MySQL 5.6åŠä»¥ä¸‹)
                        cursor.execute(f"""
                            SELECT
                                r.trx_id waiting_trx_id,
                                r.trx_mysql_thread_id waiting_thread,
                                r.trx_query waiting_query,
                                b.trx_id blocking_trx_id,
                                b.trx_mysql_thread_id blocking_thread,
                                b.trx_query blocking_query,
                                TIMESTAMPDIFF(SECOND, r.trx_wait_started, NOW()) as wait_seconds
                            FROM information_schema.innodb_lock_waits w
                            INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
                            INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id
                            WHERE TIMESTAMPDIFF(SECOND, r.trx_wait_started, NOW()) >= {min_wait_seconds}
                        """)
                        results = cursor.fetchall()

                        for row in results:
                            blocking_list.append({
                                'instance_id': instance['id'],
                                'db_project': instance['db_project'],
                                'db_ip': instance['db_ip'],
                                'db_port': instance['db_port'],
                                'blocked_thread': row['waiting_thread'],
                                'blocked_sql': row['waiting_query'] or '',
                                'blocking_thread': row['blocking_thread'],
                                'blocking_sql': row['blocking_query'] or '',
                                'wait_time': f"{row['wait_seconds']}ç§’",
                                'kill_command': f"KILL {row['blocking_thread']}",
                                'detection_method': 'information_schema.innodb_lock_waits'
                            })

                target_conn.close()

            except Exception as e:
                logger.error(f"è·å–å®ä¾‹{instance['db_project']}é˜»å¡æŸ¥è¯¢å¤±è´¥: {e}")
                continue

        return jsonify({
            'success': True,
            'data': blocking_list,
            'total_count': len(blocking_list)
        })

    except Exception as e:
        logger.error(f"è·å–é˜»å¡æŸ¥è¯¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/replication_status', methods=['GET'])
def get_replication_status():
    """è·å–ä¸»ä»å¤åˆ¶çŠ¶æ€"""
    try:
        instance_id = request.args.get('instance_id', type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–å®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE status = 1
                """)
                instances = cursor.fetchall()

        conn.close()

        replication_list = []
        for instance in instances:
            if not instance or instance['db_type'] != 'MySQL':
                continue

            try:
                target_conn = pymysql.connect(
                    host=instance['db_ip'],
                    port=instance['db_port'],
                    user=instance['db_user'],
                    password=instance['db_password'],
                    connect_timeout=3,
                    cursorclass=pymysql.cursors.DictCursor
                )

                with target_conn.cursor() as cursor:
                    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†ä¸»ä»å¤åˆ¶
                    cursor.execute("SHOW SLAVE STATUS")
                    slave_status = cursor.fetchone()

                    if slave_status:
                        replication_list.append({
                            'instance_id': instance['id'],
                            'db_project': instance['db_project'],
                            'db_ip': instance['db_ip'],
                            'db_port': instance['db_port'],
                            'master_host': slave_status.get('Master_Host'),
                            'master_port': slave_status.get('Master_Port'),
                            'slave_io_running': slave_status.get('Slave_IO_Running'),
                            'slave_sql_running': slave_status.get('Slave_SQL_Running'),
                            'seconds_behind_master': slave_status.get('Seconds_Behind_Master'),
                            'last_io_error': slave_status.get('Last_IO_Error') or '',
                            'last_sql_error': slave_status.get('Last_SQL_Error') or '',
                            'replication_healthy': (
                                slave_status.get('Slave_IO_Running') == 'Yes' and
                                slave_status.get('Slave_SQL_Running') == 'Yes' and
                                (slave_status.get('Seconds_Behind_Master') is not None and
                                 slave_status.get('Seconds_Behind_Master') < 60)
                            ),
                            'lag_warning': (
                                slave_status.get('Seconds_Behind_Master') is not None and
                                slave_status.get('Seconds_Behind_Master') > 10
                            )
                        })

                target_conn.close()

            except Exception as e:
                logger.error(f"è·å–å®ä¾‹{instance['db_project']}å¤åˆ¶çŠ¶æ€å¤±è´¥: {e}")
                continue

        return jsonify({
            'success': True,
            'data': replication_list,
            'total_count': len(replication_list)
        })

    except Exception as e:
        logger.error(f"è·å–å¤åˆ¶çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/alwayson_status', methods=['GET'])
def get_alwayson_status():
    """è·å–SQL Server Always Onå¯ç”¨æ€§ç»„çŠ¶æ€"""
    try:
        instance_id = request.args.get('instance_id', type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–å®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type, instance_name
                    FROM db_instance_info WHERE status = 1
                """)
                instances = cursor.fetchall()

        conn.close()

        alwayson_list = []
        for instance in instances:
            if not instance or instance['db_type'] not in ['SQLServer', 'SQL Server']:
                continue

            try:
                # å¯¼å…¥SQL Serveré‡‡é›†å™¨
                import sys
                sys.path.insert(0, 'scripts')
                from sqlserver_collector import SQLServerCollector

                collector = SQLServerCollector(instance)
                status_list = collector.get_alwayson_status()

                for status in status_list:
                    alwayson_list.append({
                        'instance_id': instance['id'],
                        'db_project': instance['db_project'],
                        'db_ip': instance['db_ip'],
                        'db_port': instance['db_port'],
                        'ag_name': status['ag_name'],
                        'replica_server': status['replica_server'],
                        'availability_mode': status['availability_mode'],
                        'failover_mode': status['failover_mode'],
                        'role': status['role'],
                        'connected_state': status['connected_state'],
                        'sync_health': status['sync_health'],
                        'database_name': status['database_name'],
                        'sync_state': status['sync_state'],
                        'db_sync_health': status['db_sync_health'],
                        'log_send_queue_kb': status['log_send_queue_kb'],
                        'log_send_rate_kb': status['log_send_rate_kb'],
                        'redo_queue_kb': status['redo_queue_kb'],
                        'redo_rate_kb': status['redo_rate_kb'],
                        'lag_seconds': status['lag_seconds'],
                        'is_suspended': status['is_suspended'],
                        'suspend_reason': status['suspend_reason'],
                        'is_healthy': status['is_healthy'],
                        'lag_warning': status['lag_seconds'] > 10  # å»¶è¿Ÿè¶…è¿‡10ç§’å‘Šè­¦
                    })

            except Exception as e:
                logger.error(f"è·å–å®ä¾‹{instance['db_project']} Always OnçŠ¶æ€å¤±è´¥: {e}")
                continue

        return jsonify({
            'success': True,
            'data': alwayson_list,
            'total_count': len(alwayson_list)
        })

    except Exception as e:
        logger.error(f"è·å–Always OnçŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/realtime_sql', methods=['GET'])
def get_realtime_sql():
    """è·å–å½“å‰æ­£åœ¨è¿è¡Œçš„SQLï¼ˆå®æ—¶ï¼‰"""
    try:
        instance_id = request.args.get('instance_id', type=int)
        min_seconds = request.args.get('min_seconds', 5, type=int)

        # è·å–ç›‘æ§æ•°æ®åº“è¿æ¥
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        # è·å–å®ä¾‹åˆ—è¡¨
        with conn.cursor() as cursor:
            if instance_id:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type
                    FROM db_instance_info WHERE id = %s AND status = 1
                """, (instance_id,))
                instances = [cursor.fetchone()]
            else:
                cursor.execute("""
                    SELECT id, db_project, db_ip, db_port, db_user, db_password, db_type
                    FROM db_instance_info WHERE status = 1
                """)
                instances = cursor.fetchall()

        conn.close()

        # æ”¶é›†æ‰€æœ‰å®ä¾‹çš„å®æ—¶SQL
        all_sqls = []
        for instance in instances:
            if not instance:
                continue

            try:
                if instance['db_type'] == 'MySQL':
                    # è¿æ¥ç›®æ ‡MySQLå®ä¾‹
                    target_conn = pymysql.connect(
                        host=instance['db_ip'],
                        port=instance['db_port'],
                        user=instance['db_user'],
                        password=instance['db_password'],
                        connect_timeout=3,
                        cursorclass=pymysql.cursors.DictCursor
                    )

                    with target_conn.cursor() as cursor:
                        cursor.execute("""
                            SELECT
                                id as session_id,
                                user as username,
                                host as machine,
                                db as database_name,
                                command,
                                time as elapsed_seconds,
                                state,
                                info as sql_text
                            FROM information_schema.processlist
                            WHERE command != 'Sleep'
                              AND time >= %s
                              AND info IS NOT NULL
                              AND id != CONNECTION_ID()
                            ORDER BY time DESC
                        """, (min_seconds,))

                        results = cursor.fetchall()

                        for row in results:
                            all_sqls.append({
                                'session_id': str(row['session_id']),
                                'db_instance_id': instance['id'],
                                'db_project': instance['db_project'],
                                'db_ip': instance['db_ip'],
                                'db_port': instance['db_port'],
                                'db_type': instance['db_type'],
                                'username': row['username'],
                                'machine': row['machine'],
                                'database_name': row['database_name'],
                                'elapsed_seconds': row['elapsed_seconds'] or 0,
                                'status': row['state'] or 'ACTIVE',
                                'sql_text': row['sql_text'] or ''
                            })

                    target_conn.close()

                elif instance['db_type'] in ['SQLServer', 'SQL Server']:
                    # è¿æ¥ç›®æ ‡SQL Serverå®ä¾‹
                    import pyodbc
                    conn_str = (
                        f"DRIVER={{ODBC Driver 18 for SQL Server}};"
                        f"SERVER={instance['db_ip']},{instance['db_port']};"
                        f"UID={instance['db_user']};"
                        f"PWD={instance['db_password']};"
                        f"Encrypt=no;"
                        f"TrustServerCertificate=yes;"
                        f"Timeout=5;"
                    )
                    target_conn = pyodbc.connect(conn_str)
                    cursor = target_conn.cursor()

                    # æŸ¥è¯¢SQL Serverçš„æ…¢SQLï¼ˆæ’é™¤CDCå’Œç³»ç»Ÿä½œä¸šï¼‰
                    cursor.execute(f"""
                        SELECT
                            r.session_id,
                            DATEDIFF(SECOND, r.start_time, GETDATE()) as elapsed_seconds,
                            r.status,
                            r.command,
                            s.login_name as username,
                            s.host_name as machine,
                            DB_NAME(r.database_id) as database_name,
                            t.text as sql_text
                        FROM sys.dm_exec_requests r
                        LEFT JOIN sys.dm_exec_sessions s ON r.session_id = s.session_id
                        CROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t
                        WHERE r.session_id != @@SPID
                          AND DATEDIFF(SECOND, r.start_time, GETDATE()) >= {min_seconds}
                          AND t.text IS NOT NULL
                          AND t.text NOT LIKE '%sp_server_diagnostics%'
                          AND t.text NOT LIKE '%sp_cdc_%'
                          AND (s.program_name NOT LIKE '%SQLAgent%' OR s.program_name IS NULL)
                        ORDER BY elapsed_seconds DESC
                    """)

                    results = cursor.fetchall()

                    for row in results:
                        all_sqls.append({
                            'session_id': str(row.session_id),
                            'db_instance_id': instance['id'],
                            'db_project': instance['db_project'],
                            'db_ip': instance['db_ip'],
                            'db_port': instance['db_port'],
                            'db_type': instance['db_type'],
                            'username': row.username or '',
                            'machine': row.machine or '',
                            'database_name': row.database_name or '',
                            'elapsed_seconds': row.elapsed_seconds or 0,
                            'status': row.status or 'ACTIVE',
                            'sql_text': row.sql_text or ''
                        })

                    cursor.close()
                    target_conn.close()

            except Exception as e:
                logger.error(f"è·å–å®ä¾‹{instance['db_project']}å®æ—¶SQLå¤±è´¥: {e}")
                continue

        # ç»Ÿè®¡ä¿¡æ¯
        total_count = len(all_sqls)
        max_seconds = max([sql['elapsed_seconds'] for sql in all_sqls]) if all_sqls else 0
        blocked_count = 0  # æš‚ä¸ç»Ÿè®¡é˜»å¡ä¼šè¯

        return jsonify({
            'success': True,
            'data': all_sqls,
            'stats': {
                'total_count': total_count,
                'max_seconds': max_seconds,
                'blocked_count': blocked_count
            }
        })

    except Exception as e:
        logger.error(f"è·å–å®æ—¶SQLå¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/kill_session', methods=['POST'])
def kill_session():
    """ç»ˆæ­¢ä¼šè¯"""
    try:
        data = request.get_json()
        instance_id = data.get('instance_id')
        session_id = data.get('session_id')
        db_type = data.get('db_type', 'MySQL')

        if not instance_id or not session_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400

        # è·å–å®ä¾‹è¿æ¥ä¿¡æ¯
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'ç›‘æ§æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT db_ip, db_port, db_user, db_password, db_type
                FROM db_instance_info WHERE id = %s
            """, (instance_id,))
            instance = cursor.fetchone()

        conn.close()

        if not instance:
            return jsonify({'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}), 404

        # è¿æ¥ç›®æ ‡æ•°æ®åº“å¹¶æ‰§è¡ŒKILL
        if instance['db_type'] == 'MySQL':
            import pymysql
            target_conn = pymysql.connect(
                host=instance['db_ip'],
                port=instance['db_port'],
                user=instance['db_user'],
                password=instance['db_password'],
                connect_timeout=5
            )

            try:
                with target_conn.cursor() as cursor:
                    cursor.execute(f"KILL {session_id}")
                    target_conn.commit()

                logger.info(f"æˆåŠŸç»ˆæ­¢ä¼šè¯: {instance['db_ip']}:{instance['db_port']} Session#{session_id}")
                return jsonify({'success': True, 'message': f'ä¼šè¯ {session_id} å·²æˆåŠŸç»ˆæ­¢'})
            finally:
                target_conn.close()

        elif instance['db_type'] == 'SQL Server':
            import pyodbc
            conn_str = f"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={instance['db_ip']},{instance['db_port']};UID={instance['db_user']};PWD={instance['db_password']};Encrypt=no;TrustServerCertificate=yes"
            target_conn = pyodbc.connect(conn_str, timeout=5)

            try:
                cursor = target_conn.cursor()
                cursor.execute(f"KILL {session_id}")
                target_conn.commit()

                logger.info(f"æˆåŠŸç»ˆæ­¢ä¼šè¯: {instance['db_ip']}:{instance['db_port']} SPID#{session_id}")
                return jsonify({'success': True, 'message': f'ä¼šè¯ {session_id} å·²æˆåŠŸç»ˆæ­¢'})
            finally:
                target_conn.close()

        else:
            return jsonify({'success': False, 'error': f'ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {instance["db_type"]}'}), 400

    except Exception as e:
        logger.error(f"ç»ˆæ­¢ä¼šè¯å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ============ Prometheus API ============
@app.route('/api/prometheus/metrics/<instance_ip>')
def prometheus_metrics(instance_ip):
    """è·å–æŒ‡å®šå®ä¾‹çš„PrometheusæŒ‡æ ‡"""
    try:
        config = load_config()
        prom_config = config.get('prometheus', {})

        # æ£€æŸ¥Prometheusæ˜¯å¦å¯ç”¨
        if not prom_config.get('enabled', False):
            return jsonify({'success': False, 'error': 'Prometheusæœªå¯ç”¨'}), 400

        # åˆ›å»ºPrometheuså®¢æˆ·ç«¯
        prom_url = prom_config.get('url', 'http://192.168.98.4:9090')
        timeout = prom_config.get('timeout', 5)
        prom = PrometheusClient(prom_url, timeout)

        # æ£€æŸ¥å¥åº·çŠ¶æ€
        if not prom.check_health():
            return jsonify({'success': False, 'error': 'PrometheusæœåŠ¡ä¸å¯ç”¨'}), 503

        # è·å–å®ä¾‹æŒ‡æ ‡
        metrics = prom.get_instance_metrics(instance_ip)

        return jsonify({
            'success': True,
            'data': metrics
        })

    except Exception as e:
        logger.error(f"è·å–PrometheusæŒ‡æ ‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/prometheus/trends/<instance_ip>')
def prometheus_trends(instance_ip):
    """è·å–æŒ‡å®šå®ä¾‹çš„è¶‹åŠ¿æ•°æ®"""
    try:
        config = load_config()
        prom_config = config.get('prometheus', {})

        if not prom_config.get('enabled', False):
            return jsonify({'success': False, 'error': 'Prometheusæœªå¯ç”¨'}), 400

        # è·å–æŸ¥è¯¢å‚æ•°
        hours = request.args.get('hours', type=int, default=24)

        # åˆ›å»ºPrometheuså®¢æˆ·ç«¯
        prom_url = prom_config.get('url', 'http://192.168.98.4:9090')
        timeout = prom_config.get('timeout', 5)
        prom = PrometheusClient(prom_url, timeout)

        # è·å–è¶‹åŠ¿æ•°æ®
        trends = prom.get_instance_trends(instance_ip, hours)

        return jsonify({
            'success': True,
            'data': trends
        })

    except Exception as e:
        logger.error(f"è·å–Prometheusè¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/prometheus/query', methods=['POST'])
def prometheus_query():
    """æ‰§è¡Œè‡ªå®šä¹‰PromQLæŸ¥è¯¢"""
    try:
        config = load_config()
        prom_config = config.get('prometheus', {})

        if not prom_config.get('enabled', False):
            return jsonify({'success': False, 'error': 'Prometheusæœªå¯ç”¨'}), 400

        # è·å–æŸ¥è¯¢å‚æ•°
        data = request.get_json()
        promql = data.get('query')
        query_type = data.get('type', 'instant')  # instant æˆ– range

        if not promql:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘æŸ¥è¯¢è¯­å¥'}), 400

        # åˆ›å»ºPrometheuså®¢æˆ·ç«¯
        prom_url = prom_config.get('url', 'http://192.168.98.4:9090')
        timeout = prom_config.get('timeout', 5)
        prom = PrometheusClient(prom_url, timeout)

        # æ‰§è¡ŒæŸ¥è¯¢
        if query_type == 'range':
            start = data.get('start')
            end = data.get('end')
            step = data.get('step', '15s')

            if not start or not end:
                return jsonify({'success': False, 'error': 'èŒƒå›´æŸ¥è¯¢éœ€è¦startå’Œendå‚æ•°'}), 400

            result = prom.query_range(promql, start, end, step)
        else:
            result = prom.query(promql)

        if result:
            return jsonify({
                'success': True,
                'data': result
            })
        else:
            return jsonify({'success': False, 'error': 'æŸ¥è¯¢å¤±è´¥'}), 500

    except Exception as e:
        logger.error(f"æ‰§è¡ŒPrometheusæŸ¥è¯¢å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/prometheus/health')
def prometheus_health():
    """æ£€æŸ¥PrometheusæœåŠ¡å¥åº·çŠ¶æ€"""
    try:
        config = load_config()
        prom_config = config.get('prometheus', {})

        if not prom_config.get('enabled', False):
            return jsonify({
                'success': True,
                'enabled': False,
                'healthy': False,
                'message': 'Prometheusæœªå¯ç”¨'
            })

        prom_url = prom_config.get('url', 'http://192.168.98.4:9090')
        timeout = prom_config.get('timeout', 5)
        prom = PrometheusClient(prom_url, timeout)

        healthy = prom.check_health()

        return jsonify({
            'success': True,
            'enabled': True,
            'healthy': healthy,
            'url': prom_url
        })

    except Exception as e:
        logger.error(f"æ£€æŸ¥Prometheuså¥åº·çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/prometheus/sqlserver/metrics/<instance_ip>')
def prometheus_sqlserver_metrics(instance_ip):
    """è·å–SQL Serverå®ä¾‹çš„PrometheusæŒ‡æ ‡"""
    try:
        config = load_config()
        prom_config = config.get('prometheus', {})

        # æ£€æŸ¥Prometheusæ˜¯å¦å¯ç”¨
        if not prom_config.get('enabled', False):
            return jsonify({'success': False, 'error': 'Prometheusæœªå¯ç”¨'}), 400

        # åˆ›å»ºPrometheuså®¢æˆ·ç«¯
        prom_url = prom_config.get('url', 'http://192.168.98.4:9090')
        timeout = prom_config.get('timeout', 5)
        prom = PrometheusClient(prom_url, timeout)

        # æ£€æŸ¥å¥åº·çŠ¶æ€
        if not prom.check_health():
            return jsonify({'success': False, 'error': 'PrometheusæœåŠ¡ä¸å¯ç”¨'}), 503

        # è·å–SQL Serverå®ä¾‹æŒ‡æ ‡
        metrics = prom.get_sqlserver_instance_metrics(instance_ip)

        return jsonify({
            'success': True,
            'data': metrics
        })

    except Exception as e:
        logger.error(f"è·å–SQL Server PrometheusæŒ‡æ ‡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


# ==================== DBAæ ¸å¿ƒåŠŸèƒ½API ====================


# ==================== SQLæŒ‡çº¹èšåˆAPI ====================

@app.route('/api/sql-fingerprint/stats')
def get_sql_fingerprint_stats():
    """è·å–SQLæŒ‡çº¹ç»Ÿè®¡æ•°æ®"""
    try:
        hours = request.args.get('hours', 24, type=int)
        limit = request.args.get('limit', 20, type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # è·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„SQLæŒ‡çº¹ç»Ÿè®¡
            sql = """
                SELECT
                    fps.fingerprint,
                    fps.sql_template,
                    fps.sql_type,
                    fps.tables_involved,
                    fps.occurrence_count,
                    fps.avg_elapsed_seconds,
                    fps.max_elapsed_seconds,
                    fps.avg_rows_examined,
                    fps.full_scan_count,
                    fps.has_index_suggestion,
                    fps.last_seen
                FROM sql_fingerprint_stats fps
                WHERE fps.last_seen >= DATE_SUB(NOW(), INTERVAL %s HOUR)
                ORDER BY fps.occurrence_count DESC, fps.avg_elapsed_seconds DESC
                LIMIT %s
            """

            cursor.execute(sql, (hours, limit))
            results = cursor.fetchall()

            # æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´
            for row in results:
                if isinstance(row.get('last_seen'), datetime):
                    row['last_seen'] = row['last_seen'].strftime('%Y-%m-%d %H:%M:%S')

            return jsonify({
                'success': True,
                'data': results,
                'total': len(results)
            })

    except Exception as e:
        logger.error(f"è·å–SQLæŒ‡çº¹ç»Ÿè®¡å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.route('/api/sql-fingerprint/<fingerprint>/detail')
def get_fingerprint_detail(fingerprint):
    """è·å–ç‰¹å®šSQLæŒ‡çº¹çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # è·å–æŒ‡çº¹ç»Ÿè®¡ä¿¡æ¯
            cursor.execute("""
                SELECT * FROM sql_fingerprint_stats
                WHERE fingerprint = %s
            """, (fingerprint,))
            stats = cursor.fetchone()

            if not stats:
                return jsonify({'success': False, 'error': 'æŒ‡çº¹ä¸å­˜åœ¨'}), 404

            # è·å–æœ€è¿‘çš„æ‰§è¡Œå®ä¾‹
            cursor.execute("""
                SELECT
                    l.id,
                    l.sql_text,
                    l.elapsed_seconds,
                    l.rows_examined,
                    l.detect_time,
                    i.db_project
                FROM long_running_sql_log l
                LEFT JOIN db_instance_info i ON l.db_instance_id = i.id
                WHERE l.sql_fingerprint = %s
                ORDER BY l.detect_time DESC
                LIMIT 10
            """, (fingerprint,))
            recent_sqls = cursor.fetchall()

            # è·å–ç´¢å¼•å»ºè®®
            cursor.execute("""
                SELECT * FROM index_suggestion
                WHERE sql_fingerprint = %s AND status = 'pending'
                ORDER BY benefit_score DESC
            """, (fingerprint,))
            suggestions = cursor.fetchall()

            # è·å–æ‰§è¡Œè®¡åˆ’
            cursor.execute("""
                SELECT * FROM sql_execution_plan
                WHERE sql_fingerprint = %s
                ORDER BY created_at DESC
                LIMIT 1
            """, (fingerprint,))
            plan = cursor.fetchone()

            # æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´
            for row in [stats] + recent_sqls + suggestions + ([plan] if plan else []):
                for k, v in row.items():
                    if isinstance(v, datetime):
                        row[k] = v.strftime('%Y-%m-%d %H:%M:%S')

            return jsonify({
                'success': True,
                'data': {
                    'stats': stats,
                    'recent_sqls': recent_sqls,
                    'index_suggestions': suggestions,
                    'execution_plan': plan
                }
            })

    except Exception as e:
        logger.error(f"è·å–SQLæŒ‡çº¹è¯¦æƒ…å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.route('/api/sql-fingerprint/update', methods=['POST'])
def update_sql_fingerprint():
    """æ›´æ–°SQLæŒ‡çº¹ç»Ÿè®¡ï¼ˆç”±åå°é‡‡é›†ä»»åŠ¡è°ƒç”¨ï¼‰"""
    try:
        data = request.get_json()
        sql_id = data.get('sql_id')

        if not sql_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘sql_idå‚æ•°'}), 400

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # è·å–SQLä¿¡æ¯
            cursor.execute("""
                SELECT sql_text, elapsed_seconds, rows_examined, full_table_scan
                FROM long_running_sql_log
                WHERE id = %s
            """, (sql_id,))
            sql_info = cursor.fetchone()

            if not sql_info:
                return jsonify({'success': False, 'error': 'SQLè®°å½•ä¸å­˜åœ¨'}), 404

            # ç”ŸæˆæŒ‡çº¹
            sql_text = sql_info['sql_text']
            fingerprint = SQLFingerprint.generate(sql_text)
            sql_template = SQLFingerprint.normalize(sql_text)
            metadata = SQLFingerprint.extract_metadata(sql_text)

            # æ›´æ–°long_running_sql_logè¡¨çš„æŒ‡çº¹å­—æ®µ
            cursor.execute("""
                UPDATE long_running_sql_log
                SET sql_fingerprint = %s
                WHERE id = %s
            """, (fingerprint, sql_id))

            # æ›´æ–°æˆ–æ’å…¥æŒ‡çº¹ç»Ÿè®¡
            cursor.execute("""
                INSERT INTO sql_fingerprint_stats (
                    fingerprint, sql_template, sql_type, tables_involved,
                    first_seen, last_seen, occurrence_count,
                    total_elapsed_seconds, avg_elapsed_seconds,
                    max_elapsed_seconds, min_elapsed_seconds,
                    total_rows_examined, avg_rows_examined,
                    full_scan_count
                ) VALUES (
                    %s, %s, %s, %s, NOW(), NOW(), 1,
                    %s, %s, %s, %s, %s, %s, %s
                ) ON DUPLICATE KEY UPDATE
                    last_seen = NOW(),
                    occurrence_count = occurrence_count + 1,
                    total_elapsed_seconds = total_elapsed_seconds + %s,
                    avg_elapsed_seconds = (total_elapsed_seconds + %s) / (occurrence_count + 1),
                    max_elapsed_seconds = GREATEST(max_elapsed_seconds, %s),
                    min_elapsed_seconds = LEAST(min_elapsed_seconds, %s),
                    total_rows_examined = total_rows_examined + %s,
                    avg_rows_examined = (total_rows_examined + %s) / (occurrence_count + 1),
                    full_scan_count = full_scan_count + %s
            """, (
                fingerprint, sql_template, metadata['sql_type'],
                ','.join(metadata['tables']),
                sql_info['elapsed_seconds'], sql_info['elapsed_seconds'],
                sql_info['elapsed_seconds'], sql_info['elapsed_seconds'],
                sql_info['rows_examined'] or 0, sql_info['rows_examined'] or 0,
                1 if sql_info['full_table_scan'] else 0,
                # ON DUPLICATE KEY UPDATEéƒ¨åˆ†
                sql_info['elapsed_seconds'], sql_info['elapsed_seconds'],
                sql_info['elapsed_seconds'], sql_info['elapsed_seconds'],
                sql_info['rows_examined'] or 0, sql_info['rows_examined'] or 0,
                1 if sql_info['full_table_scan'] else 0
            ))

            conn.commit()

            return jsonify({
                'success': True,
                'fingerprint': fingerprint,
                'sql_template': sql_template
            })

    except Exception as e:
        logger.error(f"æ›´æ–°SQLæŒ‡çº¹å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()




# ==================== SQLæ‰§è¡Œè®¡åˆ’åˆ†æAPI ====================

@app.route('/api/sql-explain/analyze', methods=['POST'])
def analyze_sql_explain():
    """åˆ†æSQLæ‰§è¡Œè®¡åˆ’"""
    conn = None
    target_conn = None
    try:
        data = request.get_json()
        sql_text = data.get('sql_text')
        db_instance_id = data.get('db_instance_id')

        if not sql_text or not db_instance_id:
            return jsonify({'success': False, 'error': 'ç¼ºå°‘å¿…éœ€å‚æ•°'}), 400

        # è·å–å®ä¾‹ä¿¡æ¯
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT * FROM db_instance_info WHERE id = %s
            """, (db_instance_id,))
            instance = cursor.fetchone()

            if not instance:
                return jsonify({'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}), 404

        # è¿æ¥åˆ°ç›®æ ‡æ•°æ®åº“
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®åº“ï¼ŒMySQLè¿æ¥åˆ°information_schemaï¼ŒSQLServerè¿æ¥åˆ°master
        db_name = instance.get('db_name') or ('information_schema' if instance['db_type'] == 'MySQL' else 'master')
        target_conn = pymysql.connect(
            host=instance['db_ip'],
            port=instance['db_port'],
            user=instance['db_user'],
            password=instance['db_password'],
            database=db_name,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        try:
            # æ‰§è¡Œåˆ†æ
            analyzer = SQLExplainAnalyzer(target_conn)
            result = analyzer.analyze_sql(sql_text, instance['db_type'])

            if result['success']:
                # ä¿å­˜åˆ†æç»“æœ
                fingerprint = SQLFingerprint.generate(sql_text)

                with conn.cursor() as cursor:
                    cursor.execute("""
                        INSERT INTO sql_execution_plan (
                            sql_fingerprint, db_instance_id,
                            plan_json, has_full_scan, has_temp_table, has_filesort,
                            estimated_rows, analysis_result
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                    """, (
                        fingerprint, db_instance_id,
                        json.dumps(result.get('plan_json', {})),
                        1 if result['has_full_scan'] else 0,
                        1 if result['has_temp_table'] else 0,
                        1 if result['has_filesort'] else 0,
                        sum(i.get('rows', 0) for i in result.get('issues', [])),
                        json.dumps(result.get('issues', []))
                    ))

                    plan_id = cursor.lastrowid

                    # ä¿å­˜ç´¢å¼•å»ºè®®
                    for suggestion in result.get('index_suggestions', []):
                        cursor.execute("""
                            INSERT INTO index_suggestion (
                                sql_fingerprint, db_instance_id,
                                table_name, suggested_columns,
                                create_statement, benefit_score
                            ) VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            fingerprint, db_instance_id,
                            suggestion.get('table', ''),
                            ','.join(suggestion.get('columns', [])),
                            suggestion.get('create_statement', ''),
                            80.0 if suggestion['type'] == 'CREATE_INDEX' else 60.0
                        ))

                    conn.commit()

                # ç”ŸæˆæŠ¥å‘Š
                report = analyzer.generate_optimization_report(result)

                return jsonify({
                    'success': True,
                    'plan_id': plan_id,
                    'analysis': result,
                    'report': report
                })
            else:
                return jsonify(result), 500

        finally:
            if target_conn:
                target_conn.close()

    except Exception as e:
        logger.error(f"åˆ†æSQLæ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.route('/api/sql-explain/batch-analyze', methods=['POST'])
def batch_analyze_sql_explain():
    """æ‰¹é‡åˆ†ææ…¢SQLçš„æ‰§è¡Œè®¡åˆ’"""
    try:
        hours = request.args.get('hours', 24, type=int)
        limit = request.args.get('limit', 50, type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # è·å–æœªåˆ†æçš„æ…¢SQL
            cursor.execute("""
                SELECT DISTINCT
                    l.id, l.sql_text, l.sql_fingerprint, l.db_instance_id
                FROM long_running_sql_log l
                LEFT JOIN sql_execution_plan p ON l.sql_fingerprint = p.sql_fingerprint
                WHERE l.detect_time >= DATE_SUB(NOW(), INTERVAL %s HOUR)
                  AND l.elapsed_seconds > 1
                  AND p.id IS NULL
                ORDER BY l.elapsed_seconds DESC
                LIMIT %s
            """, (hours, limit))

            sqls = cursor.fetchall()

        analyzed_count = 0
        failed_count = 0
        results = []

        for sql_record in sqls:
            try:
                # è°ƒç”¨å•ä¸ªåˆ†æAPI
                analyze_result = analyze_sql_explain_internal(
                    sql_record['sql_text'],
                    sql_record['db_instance_id']
                )

                if analyze_result['success']:
                    analyzed_count += 1
                    results.append({
                        'sql_id': sql_record['id'],
                        'fingerprint': sql_record['sql_fingerprint'],
                        'status': 'success',
                        'issues_count': len(analyze_result.get('analysis', {}).get('issues', []))
                    })
                else:
                    failed_count += 1
                    results.append({
                        'sql_id': sql_record['id'],
                        'status': 'failed',
                        'error': analyze_result.get('error')
                    })

            except Exception as e:
                failed_count += 1
                results.append({
                    'sql_id': sql_record['id'],
                    'status': 'failed',
                    'error': str(e)
                })

        return jsonify({
            'success': True,
            'total': len(sqls),
            'analyzed': analyzed_count,
            'failed': failed_count,
            'results': results
        })

    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æSQLæ‰§è¡Œè®¡åˆ’å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


def analyze_sql_explain_internal(sql_text, db_instance_id):
    """å†…éƒ¨è°ƒç”¨çš„åˆ†æå‡½æ•°ï¼ˆä¸è¿”å›Flask Responseï¼‰"""
    conn = None
    target_conn = None
    try:
        conn = get_db_connection()
        if not conn:
            return {'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}

        with conn.cursor() as cursor:
            cursor.execute("SELECT * FROM db_instance_info WHERE id = %s", (db_instance_id,))
            instance = cursor.fetchone()

        if not instance:
            return {'success': False, 'error': 'å®ä¾‹ä¸å­˜åœ¨'}

        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®åº“ï¼ŒMySQLè¿æ¥åˆ°information_schemaï¼ŒSQLServerè¿æ¥åˆ°master
        db_name = instance.get('db_name') or ('information_schema' if instance['db_type'] == 'MySQL' else 'master')
        target_conn = pymysql.connect(
            host=instance['db_ip'],
            port=instance['db_port'],
            user=instance['db_user'],
            password=instance['db_password'],
            database=db_name,
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        analyzer = SQLExplainAnalyzer(target_conn)
        result = analyzer.analyze_sql(sql_text, instance['db_type'])

        if result['success']:
            # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“...
            pass

        return result

    except Exception as e:
        return {'success': False, 'error': str(e)}
    finally:
        if target_conn:
            target_conn.close()
        if conn:
            conn.close()


@app.route('/api/index-suggestions')
def get_index_suggestions():
    """è·å–ç´¢å¼•å»ºè®®åˆ—è¡¨"""
    try:
        status = request.args.get('status', 'pending')
        limit = request.args.get('limit', 20, type=int)

        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            cursor.execute("""
                SELECT
                    s.*,
                    i.db_project,
                    fps.occurrence_count,
                    fps.avg_elapsed_seconds
                FROM index_suggestion s
                LEFT JOIN db_instance_info i ON s.db_instance_id = i.id
                LEFT JOIN sql_fingerprint_stats fps ON s.sql_fingerprint = fps.fingerprint
                WHERE s.status = %s
                ORDER BY s.benefit_score DESC, fps.occurrence_count DESC
                LIMIT %s
            """, (status, limit))

            suggestions = cursor.fetchall()

            # æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´
            for row in suggestions:
                for k, v in row.items():
                    if isinstance(v, datetime):
                        row[k] = v.strftime('%Y-%m-%d %H:%M:%S')

            return jsonify({
                'success': True,
                'data': suggestions,
                'total': len(suggestions)
            })

    except Exception as e:
        logger.error(f"è·å–ç´¢å¼•å»ºè®®å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.route('/api/index-suggestions/<int:suggestion_id>/apply', methods=['POST'])
def apply_index_suggestion(suggestion_id):
    """åº”ç”¨ç´¢å¼•å»ºè®®ï¼ˆæ‰§è¡ŒCREATE INDEXè¯­å¥ï¼‰"""
    conn = None
    target_conn = None
    try:
        conn = get_db_connection()
        if not conn:
            return jsonify({'success': False, 'error': 'æ•°æ®åº“è¿æ¥å¤±è´¥'}), 500

        with conn.cursor() as cursor:
            # è·å–ç´¢å¼•å»ºè®®
            cursor.execute("""
                SELECT s.*, i.* FROM index_suggestion s
                LEFT JOIN db_instance_info i ON s.db_instance_id = i.id
                WHERE s.id = %s
            """, (suggestion_id,))
            suggestion = cursor.fetchone()

            if not suggestion:
                return jsonify({'success': False, 'error': 'å»ºè®®ä¸å­˜åœ¨'}), 404

            if suggestion['status'] != 'pending':
                return jsonify({'success': False, 'error': 'è¯¥å»ºè®®å·²å¤„ç†'}), 400

        # è¿æ¥åˆ°ç›®æ ‡æ•°æ®åº“æ‰§è¡ŒCREATE INDEX
        # å¦‚æœæ²¡æœ‰æŒ‡å®šæ•°æ®åº“ï¼ŒMySQLè¿æ¥åˆ°information_schemaï¼ŒSQLServerè¿æ¥åˆ°master
        db_name = suggestion.get('db_name') or ('information_schema' if suggestion['db_type'] == 'MySQL' else 'master')
        target_conn = pymysql.connect(
            host=suggestion['db_ip'],
            port=suggestion['db_port'],
            user=suggestion['db_user'],
            password=suggestion['db_password'],
            database=db_name,
            charset='utf8mb4'
        )

        try:
            with target_conn.cursor() as target_cursor:
                # æ‰§è¡ŒCREATE INDEX
                create_statement = suggestion['create_statement']
                target_cursor.execute(create_statement)
                target_conn.commit()

            # æ›´æ–°çŠ¶æ€
            with conn.cursor() as cursor:
                cursor.execute("""
                    UPDATE index_suggestion
                    SET status = 'applied', applied_at = NOW()
                    WHERE id = %s
                """, (suggestion_id,))
                conn.commit()

            return jsonify({
                'success': True,
                'message': 'ç´¢å¼•åˆ›å»ºæˆåŠŸ',
                'create_statement': create_statement
            })

        finally:
            if target_conn:
                target_conn.close()

    except Exception as e:
        logger.error(f"åº”ç”¨ç´¢å¼•å»ºè®®å¤±è´¥: {e}")
        if conn:
            conn.rollback()
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        if conn:
            conn.close()


@app.errorhandler(404)
def not_found(e):
    return jsonify({'success': False, 'error': 'Not Found'}), 404

@app.errorhandler(500)
def server_error(e):
    return jsonify({'success': False, 'error': str(e)}), 500


# ==================== åå°é‡‡é›†å™¨å®šæ—¶ä»»åŠ¡ ====================

def run_mysql_collector():
    """MySQL Performance Schema é‡‡é›†å™¨"""
    try:
        config = load_config()
        mysql_config = config.get('collectors', {}).get('mysql', {})

        if not mysql_config.get('enabled', True):
            logger.debug("MySQLé‡‡é›†å™¨å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡é‡‡é›†")
            return

        threshold = mysql_config.get('threshold', 5)

        from scripts.mysql_perfschema_collector import MySQLPerfSchemaCollector, get_mysql_instances

        instances = get_mysql_instances()
        total_saved = 0

        for instance in instances:
            try:
                collector = MySQLPerfSchemaCollector(instance, threshold_seconds=threshold)
                saved = collector.collect()
                total_saved += saved
            except Exception as e:
                logger.error(f"MySQLé‡‡é›†å¤±è´¥ {instance.get('db_project')}: {e}")

        if total_saved > 0:
            logger.info(f"MySQLé‡‡é›†å®Œæˆ: {total_saved} æ¡æ…¢SQL (é˜ˆå€¼: {threshold}ç§’)")
    except Exception as e:
        logger.error(f"MySQLé‡‡é›†å™¨å¼‚å¸¸: {e}")


def run_sqlserver_collector():
    """SQL Server Query Store é‡‡é›†å™¨ (è¿‡æ»¤CDCä½œä¸š)"""
    try:
        config = load_config()
        sqlserver_config = config.get('collectors', {}).get('sqlserver', {})

        if not sqlserver_config.get('enabled', True):
            logger.debug("SQL Serveré‡‡é›†å™¨å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡é‡‡é›†")
            return

        threshold = sqlserver_config.get('threshold', 5)
        auto_enable = sqlserver_config.get('auto_enable_querystore', False)

        from scripts.sqlserver_querystore_collector import SQLServerQueryStoreCollector, get_sqlserver_instances

        instances = get_sqlserver_instances()
        total_saved = 0

        for instance in instances:
            try:
                collector = SQLServerQueryStoreCollector(instance, threshold_seconds=threshold)
                saved = collector.collect(auto_enable_querystore=auto_enable)
                total_saved += saved
            except Exception as e:
                logger.error(f"SQL Serveré‡‡é›†å¤±è´¥ {instance.get('db_project')}: {e}")

        if total_saved > 0:
            logger.info(f"SQL Serveré‡‡é›†å®Œæˆ: {total_saved} æ¡æ…¢SQL (é˜ˆå€¼: {threshold}ç§’)")
    except Exception as e:
        logger.error(f"SQL Serveré‡‡é›†å™¨å¼‚å¸¸: {e}")


def run_deadlock_collector():
    """SQL Serveræ­»é”æ£€æµ‹å™¨"""
    try:
        config = load_config()
        deadlock_config = config.get('collectors', {}).get('deadlock', {})

        if not deadlock_config.get('enabled', True):
            logger.debug("æ­»é”æ£€æµ‹å™¨å·²ç¦ç”¨ï¼Œè·³è¿‡æœ¬æ¬¡æ£€æµ‹")
            return

        logger.info("å¼€å§‹SQL Serveræ­»é”æ£€æµ‹...")
        collect_all_sqlserver_deadlocks(config['database'])
        logger.info("SQL Serveræ­»é”æ£€æµ‹å®Œæˆ")

    except Exception as e:
        logger.error(f"æ­»é”æ£€æµ‹å™¨å¼‚å¸¸: {e}")


# åˆ›å»ºåå°è°ƒåº¦å™¨
scheduler = BackgroundScheduler()

def init_scheduler():
    """åˆå§‹åŒ–è°ƒåº¦å™¨ï¼Œä»é…ç½®è¯»å–é—´éš”"""
    config = load_config()
    collectors_config = config.get('collectors', {})

    mysql_config = collectors_config.get('mysql', {})
    if mysql_config.get('enabled', True):
        interval = mysql_config.get('interval', 60)
        scheduler.add_job(func=run_mysql_collector, trigger="interval", seconds=interval,
                         id="mysql_collector", replace_existing=True)
        logger.info(f"MySQLé‡‡é›†å™¨å·²å¯åŠ¨ï¼Œé—´éš”: {interval}ç§’")

    sqlserver_config = collectors_config.get('sqlserver', {})
    if sqlserver_config.get('enabled', True):
        interval = sqlserver_config.get('interval', 60)
        scheduler.add_job(func=run_sqlserver_collector, trigger="interval", seconds=interval,
                         id="sqlserver_collector", replace_existing=True)
        logger.info(f"SQL Serveré‡‡é›†å™¨å·²å¯åŠ¨ï¼Œé—´éš”: {interval}ç§’")

    deadlock_config = collectors_config.get('deadlock', {})
    if deadlock_config.get('enabled', True):
        interval = deadlock_config.get('interval', 300)  # é»˜è®¤5åˆ†é’Ÿ
        scheduler.add_job(func=run_deadlock_collector, trigger="interval", seconds=interval,
                         id="deadlock_collector", replace_existing=True)
        logger.info(f"æ­»é”æ£€æµ‹å™¨å·²å¯åŠ¨ï¼Œé—´éš”: {interval}ç§’")

def update_collector_schedule(collector_type, enabled, interval):
    """åŠ¨æ€æ›´æ–°é‡‡é›†å™¨è°ƒåº¦"""
    job_id = f"{collector_type}_collector"

    if not enabled:
        # ç¦ç”¨é‡‡é›†å™¨ - ç§»é™¤ä»»åŠ¡
        if scheduler.get_job(job_id):
            scheduler.remove_job(job_id)
            logger.info(f"{collector_type}é‡‡é›†å™¨å·²ç¦ç”¨")
    else:
        # å¯ç”¨é‡‡é›†å™¨ - æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡
        func = run_mysql_collector if collector_type == 'mysql' else run_sqlserver_collector
        scheduler.add_job(func=func, trigger="interval", seconds=interval,
                         id=job_id, replace_existing=True)
        logger.info(f"{collector_type}é‡‡é›†å™¨å·²æ›´æ–°ï¼Œé—´éš”: {interval}ç§’")

# æ³¨å†Œé€€å‡ºæ—¶å…³é—­è°ƒåº¦å™¨
atexit.register(lambda: scheduler.shutdown())


if __name__ == '__main__':
    os.makedirs('static', exist_ok=True)
    os.makedirs('templates', exist_ok=True)

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
    init_db_pool()

    print("=" * 50)
    print("æ•°æ®åº“SQLç›‘æ§ç³»ç»Ÿ v1.3.0")
    print("=" * 50)
    print(f"è®¿é—®: http://localhost:5000")
    print(f"é…ç½®: {CONFIG_FILE}")
    print("=" * 50)
    print("ä¼˜åŒ–ç‰¹æ€§:")
    print("  [OK] æ•°æ®åº“è¿æ¥æ±  (æœ€å¤§20ä¸ªè¿æ¥)")
    print("  [OK] é…ç½®ç¼“å­˜ (60ç§’è‡ªåŠ¨åˆ·æ–°)")
    print("  [OK] Prometheusç›‘æ§ (MySQL + SQL Server)")
    print("=" * 50)
    print("åå°é‡‡é›†å™¨:")
    print("  [OK] MySQL Performance Schema é‡‡é›†å™¨ (60ç§’/æ¬¡)")
    print("  [OK] SQL Server Query Store é‡‡é›†å™¨ (60ç§’/æ¬¡)")
    print("  [OK] è‡ªåŠ¨è¿‡æ»¤CDCä½œä¸šå’Œç³»ç»ŸSQL")
    print("=" * 50)

    # åˆå§‹åŒ–å¹¶å¯åŠ¨åå°é‡‡é›†è°ƒåº¦å™¨
    init_scheduler()
    scheduler.start()
    logger.info("åå°é‡‡é›†è°ƒåº¦å™¨å·²å¯åŠ¨")

    # ç«‹å³æ‰§è¡Œä¸€æ¬¡é‡‡é›†
    config = load_config()
    collectors_config = config.get('collectors', {})
    logger.info("æ‰§è¡Œé¦–æ¬¡é‡‡é›†...")
    if collectors_config.get('mysql', {}).get('enabled', True):
        run_mysql_collector()
    if collectors_config.get('sqlserver', {}).get('enabled', True):
        run_sqlserver_collector()

    # å¯åŠ¨Flaskåº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)
